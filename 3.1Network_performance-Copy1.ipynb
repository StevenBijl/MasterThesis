{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fdcbe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with Deep Learning\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting with Deep Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff01ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import struct\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tabulate import tabulate\n",
    "import csv\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871749f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import struct\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tabulate import tabulate\n",
    "import csv\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae38eb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5774bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "275424f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31]\n"
     ]
    }
   ],
   "source": [
    "### Usable Files\n",
    "\n",
    "direction = pathlib.Path(\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Data/Detector97/Filtered/Processed/\")\n",
    "\n",
    "# direction = pathlib.Path(\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Data/Detector97/Unfiltered/Processed/\")\n",
    "\n",
    "file_list = list(direction.iterdir())\n",
    "\n",
    "file_num = len(file_list)\n",
    "\n",
    "num_test_files = 2\n",
    "eighty = round(0.8*file_num)\n",
    "twenty = file_num-eighty - num_test_files\n",
    "\n",
    "list_of_file_ids_train = np.arange(eighty, dtype=int)\n",
    "print(list_of_file_ids_train)\n",
    "list_of_file_ids_val = np.arange(eighty,eighty+twenty-num_test_files, dtype=int)\n",
    "\n",
    "list_of_file_ids_test =np.arange(file_num-num_test_files,file_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef1a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functions to process the data\n",
    "\n",
    "        ### Unnormalization of each signal individually\n",
    "def Unnormalized(batch_signals):\n",
    "        \n",
    "        return batch_signals\n",
    "        \n",
    "        ### Normalization of each signal individually\n",
    "def Normalized(batch_signals):\n",
    "\n",
    "        for i in range(len(batch_signals)):\n",
    "            batch_signals[i] = batch_signals[i]/np.max(batch_signals[i])\n",
    "            \n",
    "        return batch_signals\n",
    "            \n",
    "        \n",
    "        ### Normalization of the entire value by one common denominator      \n",
    "def Denominator(batch_signals):  \n",
    "    \n",
    "        denominator = 3953.48\n",
    "        batch_signals = batch_signals/denominator\n",
    "        \n",
    "        return batch_signals\n",
    "\n",
    "\n",
    "##### Class\n",
    "\n",
    "class TrainDataset(tf.data.Dataset):\n",
    "\n",
    "    def _generator(file_id):  \n",
    "#         print(f'Using Train Class')\n",
    "        if(file_id == 0):\n",
    "#             print(\"reshuffling\")\n",
    "            np.random.shuffle(list_of_file_ids_train)             \n",
    "\n",
    "        i_file = list_of_file_ids_train[file_id]\n",
    "\n",
    "#         print(f'file_id: {file_id}, i_file: {i_file}')\n",
    "#         print()\n",
    "        signal_filename = direction/f'{i_file+1}.h5'\n",
    "\n",
    "        \n",
    "         # Load the labels and signals from the files\n",
    "        df = pd.read_hdf(signal_filename,key=None)  \n",
    "        \n",
    "        labels1 = df.iloc[:,9].values\n",
    "        labels2 = df['ToF'].values\n",
    "        labels = labels1+labels2\n",
    "        \n",
    "        signals = df[df.columns[10:-2]].values\n",
    "        \n",
    "        \n",
    "        # Determine how many batches can be made from this file\n",
    "        num_batches = len(signals) // batch_size\n",
    "\n",
    "        # Shuffle the signals within the file\n",
    "        signal_indices = np.arange(len(signals))\n",
    "        np.random.shuffle(signal_indices)        \n",
    "        \n",
    "        # Loop through each batch in the file\n",
    "        for batch_idx in range(num_batches):\n",
    "            # Get the signals and labels for this batch\n",
    "            batch_signal_indices = signal_indices[batch_idx*batch_size:(batch_idx+1)*batch_size]      \n",
    " \n",
    "            batch_signals = signals[batch_signal_indices]\n",
    "            \n",
    "            batch_signals = Processing[process](batch_signals)\n",
    "                \n",
    "            batch_signals = batch_signals[:,:,np.newaxis] # Can also be done with signals = signals[:,:,np.newaxis]\n",
    "            batch_labels = labels[batch_signal_indices]\n",
    "\n",
    "            # Yield the signals and labels as a tuple\n",
    "            yield batch_signals, batch_labels \n",
    "             \n",
    "    def __new__(cls, file_id):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=(tf.dtypes.float64, tf.dtypes.float64),\n",
    "            output_shapes=((batch_size, 1998,1), (batch_size, )),\n",
    "            args=(file_id,)\n",
    "        )\n",
    "    \n",
    "class ValDataset(tf.data.Dataset):\n",
    "\n",
    "    def _generator(file_id):  \n",
    "#         print(f'Using Val Class')\n",
    "        i_file = list_of_file_ids_val[file_id]\n",
    "    \n",
    "        signal_filename = direction/f'{i_file+1}.h5'\n",
    "\n",
    "         # Load the labels and signals from the files\n",
    "        df = pd.read_hdf(signal_filename,key=None)    \n",
    "        \n",
    "        labels1 = df.iloc[:,9].values\n",
    "        labels2 = df['ToF'].values\n",
    "        labels = labels1+labels2\n",
    "        \n",
    "        signals = df[df.columns[10:-2]].values\n",
    "        \n",
    "        \n",
    "        # Determine how many batches can be made from this file\n",
    "        num_batches = len(signals) // batch_size\n",
    "\n",
    "        # Shuffle the signals within the file\n",
    "        signal_indices = np.arange(len(signals))\n",
    "        np.random.shuffle(signal_indices)        \n",
    "        \n",
    "        # Loop through each batch in the file\n",
    "        for batch_idx in range(num_batches):\n",
    "            # Get the signals and labels for this batch\n",
    "            batch_signal_indices = signal_indices[batch_idx*batch_size:(batch_idx+1)*batch_size]      \n",
    " \n",
    "            batch_signals = signals[batch_signal_indices]\n",
    "            \n",
    "            batch_signals = Processing[process](batch_signals)\n",
    "                \n",
    "            batch_signals = batch_signals[:,:,np.newaxis] # Can also be done with signals = signals[:,:,np.newaxis]\n",
    "            batch_labels = labels[batch_signal_indices]\n",
    "\n",
    "            # Yield the signals and labels as a tuple\n",
    "            yield batch_signals, batch_labels\n",
    "             \n",
    "    def __new__(cls, file_id):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=(tf.dtypes.float64, tf.dtypes.float64),\n",
    "            output_shapes=((batch_size, 1998,1), (batch_size, )),\n",
    "            args=(file_id,)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e9da4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499.14654541 499.83734131 499.82849121 ... 499.1619873  499.05609131\n",
      " 499.6555481 ]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_hdf(file_list[0],key=None)  \n",
    "\n",
    "labels1 = df.iloc[:,9].values\n",
    "labels2 = df['ToF'].values\n",
    "labels = labels1+labels2\n",
    "\n",
    "print(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c979446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential()\n",
    "# model.add(Conv1D(filters=16, kernel_size=5,activation='relu', input_shape=(1998, 1)))\n",
    "# model.add(Conv1D(filters=8, kernel_size=5,dilation_rate=2,activation='relu'))\n",
    "# model.add(Conv1D(filters=4, kernel_size=5,dilation_rate=2,activation='relu'))\n",
    "# model.add(Conv1D(filters=4, kernel_size=5,strides=2,activation='relu'))\n",
    "# model.add(Conv1D(filters=4, kernel_size=3,strides=2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(32,activation='relu'))\n",
    "# model.add(Dense(16,activation='relu'))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92e20fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 1994, 16)          96        \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1986, 8)           648       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1986, 8)           32        \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1978, 4)           164       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 987, 4)            84        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 987, 4)            16        \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 493, 4)            52        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1972)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                63136     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 64,773\n",
      "Trainable params: 64,749\n",
      "Non-trainable params: 24\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(Conv1D(filters=16, kernel_size=5,activation='relu', input_shape=(1998, 1)))\n",
    "model.add(Conv1D(filters=8, kernel_size=5,dilation_rate=2,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=4, kernel_size=5,dilation_rate=2,activation='relu'))\n",
    "model.add(Conv1D(filters=4, kernel_size=5,strides=2,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(filters=4, kernel_size=3,strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f780b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "# # Define the regularization parameters\n",
    "# l1 = 0.001  # L1 regularization parameter\n",
    "# l2 = 0.001  # L2 regularization parameter\n",
    "\n",
    "# # Create the model\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(filters=16, kernel_size=5, activation='relu', input_shape=(1998, 1), kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n",
    "# model.add(Conv1D(filters=8, kernel_size=5, dilation_rate=2, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv1D(filters=4, kernel_size=5, dilation_rate=2, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n",
    "# model.add(Conv1D(filters=4, kernel_size=5, strides=2, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv1D(filters=4, kernel_size=3, strides=2, kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n",
    "# model.add(Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff5031de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential()\n",
    "# model.add(Conv1D(filters=16, kernel_size=5,activation=tf.keras.layers.LeakyReLU(alpha=0.01), input_shape=(1998, 1)))\n",
    "# model.add(Conv1D(filters=8, kernel_size=5,dilation_rate=2,activation=tf.keras.layers.LeakyReLU(alpha=0.01)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv1D(filters=4, kernel_size=5,dilation_rate=2,activation=tf.keras.layers.LeakyReLU(alpha=0.01)))\n",
    "# model.add(Conv1D(filters=4, kernel_size=5,strides=2,activation=tf.keras.layers.LeakyReLU(alpha=0.01)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv1D(filters=4, kernel_size=3,strides=2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(32,activation=tf.keras.layers.LeakyReLU(alpha=0.01)))\n",
    "# model.add(Dense(16,activation=tf.keras.layers.LeakyReLU(alpha=0.01)))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f2aea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv1D, BatchNormalization, Flatten, Dense, MaxPooling1D, Add\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(filters=16, kernel_size=5, activation='relu', input_shape=(1998, 1)))\n",
    "# model.add(Conv1D(filters=16, kernel_size=5, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Conv1D(filters=32, kernel_size=5, dilation_rate=2, activation='relu'))\n",
    "# model.add(Conv1D(filters=32, kernel_size=5, dilation_rate=2, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Conv1D(filters=64, kernel_size=5, dilation_rate=2, activation='relu'))\n",
    "# model.add(Conv1D(filters=64, kernel_size=5, dilation_rate=2, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Conv1D(filters=128, kernel_size=5, dilation_rate=2, activation='relu'))\n",
    "# model.add(Conv1D(filters=128, kernel_size=5, dilation_rate=2, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89a576a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_15988\\2373516906.py:76: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_15988\\2373516906.py:76: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing options\n",
    "Processing = [Unnormalized,Normalized,Denominator]\n",
    "process = 2\n",
    "\n",
    "# Loss Function\n",
    "loss_function = ['mean_absolute_error','mean_squared_error']\n",
    "lf = 1\n",
    "\n",
    "# Training Variables\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "steps_per_epoch = eighty*5000 // batch_size\n",
    "\n",
    "# Learning Rate\n",
    "initial_lr = 1e-03\n",
    "final_lr = 1e-06\n",
    "\n",
    "# initial_lr = 1e-03\n",
    "# final_lr = 1e-03\n",
    "\n",
    "def step_decay(epoch):\n",
    "    lrate = initial_lr * (final_lr/initial_lr)**(epoch/num_epochs)\n",
    "\n",
    "    print(f'Current Learning rate: {lrate}')\n",
    "    return lrate\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=loss_function[lf], optimizer = keras.optimizers.Adam(initial_lr), metrics=['mean_absolute_error','mean_squared_error'])\n",
    "\n",
    "# Configuring training dataset\n",
    "dataset_train = tf.data.Dataset.range(eighty).interleave(\n",
    "        TrainDataset,\n",
    "        cycle_length=2,\n",
    "        num_parallel_calls=2,\n",
    "        deterministic=True).repeat().prefetch(1)\n",
    "\n",
    "\n",
    "\n",
    "# Configuring training dataset\n",
    "dataset_val = tf.data.Dataset.range(twenty-num_test_files).interleave(\n",
    "        ValDataset,\n",
    "        cycle_length=2,\n",
    "        num_parallel_calls=2,\n",
    "        deterministic=True).prefetch(1)\n",
    "\n",
    "\n",
    "# Callback Functions\n",
    "LRS = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "ES = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True,verbose=1)\n",
    "\n",
    "CSV = tf.keras.callbacks.CSVLogger(\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Data/Log.csv\",\n",
    "                                separator=\",\", append=True)\n",
    "\n",
    "MC_path = pathlib.Path(\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/model_checkpoint.h5\")\n",
    "MC = ModelCheckpoint(\n",
    "    filepath=MC_path,  # Filepath to save the model weights\n",
    "    monitor='val_loss',  # Quantity to monitor (e.g., validation loss)\n",
    "    save_best_only=True,  # Save only the best model based on the monitored quantity\n",
    "    save_weights_only=True  # Save only the model weights, not the entire model\n",
    ")\n",
    "\n",
    "callbacks = [MC,LRS,CSV]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a103f83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# history = model.fit(x=dataset_train, validation_data = dataset_val, steps_per_epoch=steps_per_epoch, epochs=num_epochs,callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab6e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Current Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import random\n",
    "\n",
    "Processing = [Unnormalized,Normalized,Denominator]\n",
    "process = 1\n",
    "\n",
    "# Lists to store the results\n",
    "mae_list = []\n",
    "mse_list = []\n",
    "mean_list = []\n",
    "std_list = []\n",
    "\n",
    "# Set the number of training runs\n",
    "num_runs = 2\n",
    "\n",
    "for run in range(num_runs):\n",
    "    # Set a new random seed for each run\n",
    "    random_seed = run  # Change this if you want a different way to generate the seed\n",
    "\n",
    "    # Set the random seed for NumPy and TensorFlow (or any other random number generators you may be using)\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    # TensorFlow random seed\n",
    "    tf.random.set_seed(random_seed)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(x=dataset_train, validation_data=dataset_val, steps_per_epoch=steps_per_epoch, epochs=num_epochs, callbacks=callbacks)\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_weights(MC_path)\n",
    "    \n",
    "    for i in list_of_file_ids_test:\n",
    "        # Load the data and perform necessary preprocessing steps\n",
    "        df = pd.read_hdf(file_list[i], key=None)\n",
    "        labels1 = df.iloc[:, 9].values\n",
    "        labels2 = df['ToF'].values\n",
    "        labels = labels1 + labels2\n",
    "        signals = df[df.columns[10:-2]].values\n",
    "        signals = Processing[process](signals)\n",
    "        signals = signals[:, :, np.newaxis]\n",
    "\n",
    "        # Make predictions using the trained model\n",
    "        predicted = model.predict(signals)\n",
    "        predicted = np.squeeze(predicted)\n",
    "        true = labels\n",
    "\n",
    "        # Calculate MAE, MSE, Mean, and STD\n",
    "        mae = mean_absolute_error(true, predicted)\n",
    "        mse = mean_squared_error(true, predicted)\n",
    "        mean = np.mean(predicted - true)\n",
    "        std = np.std(predicted - true)\n",
    "\n",
    "        # Append to the result lists\n",
    "        mae_list.append(mae)\n",
    "        mse_list.append(mse)\n",
    "        mean_list.append(mean)\n",
    "        std_list.append(std)\n",
    "\n",
    "# Calculate the average and standard deviation\n",
    "avg_mae = np.mean(mae_list)\n",
    "avg_mse = np.mean(mse_list)\n",
    "avg_mean = np.mean(mean_list)\n",
    "avg_std = np.mean(std_list)\n",
    "\n",
    "std_mae = np.std(mae_list)\n",
    "std_mse = np.std(mse_list)\n",
    "std_mean = np.std(mean_list)\n",
    "std_std = np.std(std_list)\n",
    "\n",
    "print(f\"MAE: {mae_list}, {std_mae}\")\n",
    "print(f\"MSE: {mse_list}, {std_mse}\")\n",
    "print(f\"Mean: {mean_list}, {std_mean}\")\n",
    "print(f\"STD: {std_list}, {std_std}\")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average MAE: {avg_mae:.3f} ± {std_mae:.3f}\")\n",
    "print(f\"Average MSE: {avg_mse:.3f} ± {std_mse:.3f}\")\n",
    "print(f\"Average Mean: {avg_mean:.3f} ± {std_mean:.3f}\")\n",
    "print(f\"Average STD: {avg_std:.3f} ± {std_std:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(MC_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### All Analysis Tools:::\n",
    "\n",
    "# Loss/Val vs Epochs\n",
    "loss = history.history['loss']\n",
    "val = history.history['val_loss']\n",
    "\n",
    "x = np.arange(1,len(loss))\n",
    "\n",
    "# print(f'Training: {loss} \\n')\n",
    "# print(f'Validation: {val}')\n",
    "\n",
    "print(f'Train Final: {loss[-1]}')\n",
    "print(f'Val Final: {val[-1]}')\n",
    "\n",
    "# Full Training\n",
    "plt.plot(loss,label='Train')\n",
    "plt.title(f' Model Performance')\n",
    "plt.plot(val,label='Validation')\n",
    "plt.ylabel(f'{loss_function[lf]}')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Full Training\n",
    "plt.plot(loss,label='Train')\n",
    "plt.title(f' Model Performance')\n",
    "plt.plot(val,label='Validation')\n",
    "plt.ylabel(f'{loss_function[lf]}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Full training without first epoch\n",
    "plt.plot(x,loss[1:],label='Train')\n",
    "plt.plot(x,val[1:],label='Validation')\n",
    "plt.title(f' Model Performance')\n",
    "plt.ylabel(f'{loss_function[lf]}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Full training without first two epochs\n",
    "plt.plot(x[1:],loss[2:],label='Train')\n",
    "plt.plot(x[1:],val[2:],label='Validation')\n",
    "plt.title(f' Model Performance')\n",
    "plt.ylabel(f'{loss_function[lf]}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Full training without first three epochs\n",
    "plt.plot(x[2:],loss[3:],label='Train')\n",
    "plt.plot(x[2:],val[3:],label='Validation')\n",
    "plt.title(f' Model Performance')\n",
    "plt.ylabel(f'{loss_function[lf]}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = []\n",
    "\n",
    "\n",
    "for i in list_of_file_ids_test:\n",
    "    df = pd.read_hdf(file_list[i],key=None)   \n",
    "\n",
    "    labels1 = df.iloc[:,9].values\n",
    "    labels2 = df['ToF'].values\n",
    "    labels = labels1+labels2\n",
    "\n",
    "    signals = df[df.columns[10:-2]].values\n",
    "    signals = Processing[process](signals)\n",
    "    signals = signals[:,:,np.newaxis]\n",
    "\n",
    "    # model.predict\n",
    "    predicted = model.predict(signals)\n",
    "    predicted = np.squeeze(predicted) # getting rid of extra dimension\n",
    "    true = labels\n",
    "    \n",
    "    test_loss = np.subtract(predicted,true)\n",
    "    \n",
    "    avg_test_loss = np.mean(np.abs(test_loss))\n",
    "    print(avg_test_loss)\n",
    "    avg_loss.append(avg_test_loss)\n",
    "    \n",
    "print(f'Mean Absolute Error: {np.mean(avg_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd052f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = []\n",
    "\n",
    "for i in list_of_file_ids_test:\n",
    "    df = pd.read_hdf(file_list[i], key=None)\n",
    "    \n",
    "    labels1 = df.iloc[:, 9].values\n",
    "    labels2 = df['ToF'].values\n",
    "    labels = labels1 + labels2\n",
    "    \n",
    "    signals = df[df.columns[10:-2]].values\n",
    "    signals = Processing[process](signals)\n",
    "    signals = signals[:, :, np.newaxis]\n",
    "    \n",
    "    # model.predict\n",
    "    predicted = model.predict(signals)\n",
    "    predicted = np.squeeze(predicted)  # getting rid of extra dimension\n",
    "    true = labels\n",
    "    \n",
    "    test_loss = np.subtract(predicted, true)\n",
    "    \n",
    "    # Calculate mean squared error (MSE)\n",
    "    mse = np.mean(np.square(test_loss))\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    \n",
    "    avg_loss.append(mse)\n",
    "    \n",
    "print(f'Mean Squared Error: {np.mean(avg_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccc3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([pd.read_hdf(file_list[i], key=None) for i in list_of_file_ids_test])\n",
    "unique_mass_test = df_test['A'].unique()\n",
    "\n",
    "result_dict = {}  # Dictionary to store unique_mass and corresponding avg_test_loss values\n",
    "\n",
    "# Test Set\n",
    "for mass_test in unique_mass_test:\n",
    "    print(f\"Unique Mass: {mass_test}\")\n",
    "    df_intermediate = df_test[df_test['A'] == mass_test]\n",
    "    unique_energy = df_intermediate['Z'].unique()\n",
    "\n",
    "    for energy in unique_energy:\n",
    "        print(f\"Unique Energy: {energy}\")\n",
    "        \n",
    "        df_combination = df_intermediate[df_intermediate['Z'] == energy]\n",
    "\n",
    "        labels1 = df_combination.iloc[:, 9].values\n",
    "        labels2 = df_combination['ToF'].values\n",
    "        labels = labels1 + labels2\n",
    "\n",
    "        signals = df_combination[df_combination.columns[10:-2]].values\n",
    "        signals = Processing[process](signals)\n",
    "        signals = signals[:, :, np.newaxis]\n",
    "\n",
    "        predicted = model.predict(signals)\n",
    "        predicted = np.squeeze(predicted)\n",
    "        true = labels\n",
    "\n",
    "        difference = predicted - true\n",
    "\n",
    "        # Plotting the distribution\n",
    "        plt.figure()\n",
    "        plt.hist(difference, bins=20)\n",
    "        plt.title(f\"A={mass_test}, Z={energy}, Count={len(df_combination)}\")\n",
    "        plt.xlabel(\"Difference (predicted - true)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "\n",
    "        # Calculate mean and standard deviation\n",
    "        mean = np.mean(difference)\n",
    "        std = np.std(difference)\n",
    "\n",
    "        # Create legend text with mean and standard deviation\n",
    "        legend_text = f\"Mean: {mean:.2f}\\nSTD: {std:.2f}\"\n",
    "\n",
    "        # Add legend with the mean and standard deviation\n",
    "        plt.legend([legend_text])\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        avg_test_loss = np.mean(np.abs(difference))\n",
    "        count = len(df_combination)\n",
    "\n",
    "        result_dict[(mass_test, energy)] = {\n",
    "            'avg_test_loss': avg_test_loss,\n",
    "            'count': count,\n",
    "            'mean_test_loss': mean,\n",
    "            'std_test_loss': std\n",
    "        }\n",
    "\n",
    "# Print the result dictionary\n",
    "for (mass, energy), result in result_dict.items():\n",
    "    avg_test_loss = result['avg_test_loss']\n",
    "    count = result['count']\n",
    "    mean_test_loss = result['mean_test_loss']\n",
    "    std_test_loss = result['std_test_loss']\n",
    "\n",
    "    print(f\"Unique Mass: {mass}, Unique Energy: {energy}\")\n",
    "    print(f\"Avg Test Loss: {avg_test_loss}\")\n",
    "    print(f\"Count: {count}\")\n",
    "    print(f\"Mean Test Loss: {mean_test_loss}\")\n",
    "    print(f\"Standard Deviation Test Loss: {std_test_loss}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e760a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data for scatter plots\n",
    "mass_list = []\n",
    "energy_list = []\n",
    "mean_loss_list = []\n",
    "std_loss_list = []\n",
    "\n",
    "for (mass, energy), result in result_dict.items():\n",
    "    mass_list.append(mass)\n",
    "    energy_list.append(energy)\n",
    "    mean_loss_list.append(result['mean_test_loss'])\n",
    "    std_loss_list.append(result['std_test_loss'])\n",
    "\n",
    "    # Scatter plot for Mean Test Loss\n",
    "plt.figure()\n",
    "plt.scatter(mass_list, energy_list, c=mean_loss_list, cmap='coolwarm')\n",
    "plt.colorbar(label='Mean Test Loss')\n",
    "plt.xlabel('Mass')\n",
    "plt.ylabel('Energy')\n",
    "plt.title('Scatter Plot: Mean Test Loss')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot for Standard Deviation\n",
    "plt.figure()\n",
    "plt.scatter(mass_list, energy_list, c=std_loss_list, cmap='coolwarm')\n",
    "plt.colorbar(label='Standard Deviation Test Loss')\n",
    "plt.xlabel('Mass')\n",
    "plt.ylabel('Energy')\n",
    "plt.title('Scatter Plot: Standard Deviation Test Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "from matplotlib.ticker import LogLocator\n",
    "\n",
    "# Extract the data for scatter plots\n",
    "mass_list = []\n",
    "energy_list = []\n",
    "mean_loss_list = []\n",
    "std_loss_list = []\n",
    "\n",
    "for (mass, energy), result in result_dict.items():\n",
    "    mass_list.append(mass)\n",
    "    energy_list.append(energy)\n",
    "    mean_loss_list.append(result['mean_test_loss'])\n",
    "    std_loss_list.append(result['std_test_loss'])\n",
    "\n",
    "# Apply logarithmic transformation to the values\n",
    "mean_loss_list = [np.log10(val) for val in mean_loss_list]\n",
    "std_loss_list = [np.log10(val) for val in std_loss_list]\n",
    "\n",
    "# Create a grid of subplots with 1 row and 2 columns\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Scatter plot for Mean Test Loss\n",
    "scatter_mean = axs[0].scatter(mass_list, energy_list, c=mean_loss_list, cmap='coolwarm')\n",
    "plt.colorbar(scatter_mean, ax=axs[0], label='Mean Test Loss (log scale)')\n",
    "axs[0].set_xlabel('Mass')\n",
    "axs[0].set_ylabel('Energy')\n",
    "axs[0].set_title('Scatter Plot: Mean Test Loss')\n",
    "\n",
    "# Scatter plot for Standard Deviation\n",
    "scatter_std = axs[1].scatter(mass_list, energy_list, c=std_loss_list, cmap='coolwarm')\n",
    "plt.colorbar(scatter_std, ax=axs[1], label='Standard Deviation Test Loss (log scale)')\n",
    "axs[1].set_xlabel('Mass')\n",
    "axs[1].set_ylabel('Energy')\n",
    "axs[1].set_title('Scatter Plot: Standard Deviation Test Loss')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ef84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "\n",
    "# Extract the data for scatter plots\n",
    "mass_list = []\n",
    "energy_list = []\n",
    "mean_loss_list = []\n",
    "std_loss_list = []\n",
    "\n",
    "for (mass, energy), result in result_dict.items():\n",
    "    mass_list.append(mass)\n",
    "    energy_list.append(energy)\n",
    "    mean_loss_list.append(result['mean_test_loss'])\n",
    "    std_loss_list.append(result['std_test_loss'])\n",
    "\n",
    "# Create a grid of subplots with 1 row and 2 columns\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Scatter plot for Mean Test Loss\n",
    "scatter_mean = axs[0].scatter(mass_list, energy_list, c=mean_loss_list, cmap='coolwarm',\n",
    "                              norm=colors.LogNorm())\n",
    "plt.colorbar(scatter_mean, ax=axs[0], label='Mean Test Loss')\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].set_xlabel('Mass')\n",
    "axs[0].set_ylabel('Energy')\n",
    "axs[0].set_title('Scatter Plot: Mean Test Loss')\n",
    "\n",
    "# Scatter plot for Standard Deviation\n",
    "scatter_std = axs[1].scatter(mass_list, energy_list, c=std_loss_list, cmap='coolwarm',\n",
    "                             norm=colors.LogNorm())\n",
    "plt.colorbar(scatter_std, ax=axs[1], label='Standard Deviation Test Loss')\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xlabel('Mass')\n",
    "axs[1].set_ylabel('Energy')\n",
    "axs[1].set_title('Scatter Plot: Standard Deviation Test Loss')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([pd.read_hdf(file_list[i], key=None) for i in list_of_file_ids_test])\n",
    "unique_mass_test = df_test['A'].unique()\n",
    "\n",
    "# Test Set\n",
    "for mass_test in unique_mass_test:\n",
    "    result_list = []  # List to store results as rows\n",
    "\n",
    "    df_intermediate = df_test[df_test['A'] == mass_test]\n",
    "    unique_energy = df_intermediate['Z'].unique()\n",
    "\n",
    "    for energy in unique_energy:\n",
    "        df_combination = df_intermediate[df_intermediate['Z'] == energy]\n",
    "\n",
    "        labels1 = df_combination.iloc[:, 9].values\n",
    "        labels2 = df_combination['ToF'].values\n",
    "        labels = labels1 + labels2\n",
    "\n",
    "        signals = df_combination[df_combination.columns[10:-2]].values\n",
    "        signals = Processing[process](signals)\n",
    "        signals = signals[:, :, np.newaxis]\n",
    "\n",
    "        predicted = model.predict(signals)\n",
    "        predicted = np.squeeze(predicted)\n",
    "        true = labels\n",
    "\n",
    "        difference = predicted - true\n",
    "        # Calculate mean and standard deviation\n",
    "        mean = np.mean(difference)\n",
    "        std = np.std(difference)\n",
    "\n",
    "        avg_test_loss = np.mean(np.abs(difference))\n",
    "        count = len(df_combination)\n",
    "\n",
    "        result_list.append([round(mass_test), round(energy), avg_test_loss, count, mean, std])\n",
    "\n",
    "    # Create the tabular representation for the current unique mass\n",
    "    headers = [\"Unique Mass\", \"Unique Energy\", \"Avg Test Loss\", \"Count\", \"Mean Test Loss\", \"Std Test Loss\"]\n",
    "    tabular_result = tabulate(result_list, headers=headers, floatfmt=\".3f\", tablefmt=\"github\")\n",
    "\n",
    "    # Print the tabular result for the current unique mass\n",
    "    print(f\"Tabular representation for Unique Mass: {mass_test}\")\n",
    "    print(tabular_result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef76580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group the data by unique mass and energy\n",
    "grouped_data = df_test.groupby(['A'])\n",
    "\n",
    "# Create an empty list to store the energies for each unique mass\n",
    "energies_per_mass = []\n",
    "\n",
    "# Iterate over each unique mass group\n",
    "for mass, group in grouped_data:\n",
    "    energies = group['Z'].unique()  # Get the unique energies for the current mass\n",
    "    energies_per_mass.append((mass, energies))  # Append the unique mass and energies tuple to the list\n",
    "\n",
    "# Create a list to store the average test losses for each unique mass\n",
    "avg_test_losses = []\n",
    "\n",
    "# Create an empty list to store the differences for each unique mass\n",
    "differences_per_mass = []\n",
    "\n",
    "# Iterate over the unique masses\n",
    "for i, (mass, energies) in enumerate(energies_per_mass, 1):\n",
    "    # Create a list to store the differences for the current mass\n",
    "    differences = []\n",
    "\n",
    "    # Iterate over the energies for the current mass\n",
    "    for energy in energies:\n",
    "        # Retrieve the data for the current mass and energy\n",
    "        data = df_test[(df_test['A'] == mass) & (df_test['Z'] == energy)]\n",
    "        \n",
    "        # Extract the features (signals) and labels (true values)\n",
    "        signals = data[data.columns[10:-2]].values\n",
    "        signals = Processing[process](signals)\n",
    "        signals = signals[:, :, np.newaxis]\n",
    "        labels1 = data.iloc[:, 9].values\n",
    "        labels2 = data['ToF'].values\n",
    "        labels = labels1 + labels2\n",
    "\n",
    "        # Predict the labels using the model\n",
    "        predicted = model.predict(signals)\n",
    "        predicted = np.squeeze(predicted)\n",
    "\n",
    "        # Calculate the difference between predicted and true labels\n",
    "        difference = predicted - labels\n",
    "        differences.append(difference)  # Store the difference for the current energy\n",
    "\n",
    "    differences_per_mass.append(differences)  # Store the differences for the current mass\n",
    "\n",
    "    # Calculate the average test loss for the current mass\n",
    "    avg_test_loss = np.mean(np.abs(np.concatenate(differences)))  # Concatenate the differences for all energies\n",
    "    avg_test_losses.append(avg_test_loss)  # Store the average test loss\n",
    "\n",
    "# Iterate over the unique masses and corresponding differences\n",
    "for i, (mass, differences) in enumerate(zip(energies_per_mass, differences_per_mass), 1):\n",
    "    energies = mass[1]  # Get the energies for the current mass\n",
    "\n",
    "    # Sort the energies in ascending order\n",
    "    sorted_energies = np.sort(energies)\n",
    "\n",
    "    # Get the indices to reorder the differences based on the sorted energies\n",
    "    indices = np.argsort(energies)\n",
    "    reordered_differences = [differences[index] for index in indices]\n",
    "\n",
    "    plt.figure(figsize=(150, 6))  # Adjust the figure size as needed\n",
    "    plt.subplot(1, len(energies_per_mass), i)  # Create a subplot for each unique mass\n",
    "    plt.boxplot(reordered_differences)\n",
    "    plt.title(f\"Unique Mass: {mass[0]}\")\n",
    "    plt.xlabel(\"Energy\")\n",
    "    plt.ylabel(\"Difference (Predicted - True)\")\n",
    "\n",
    "    # Set the x-axis tick labels to the sorted energies\n",
    "    plt.xticks(range(1, len(sorted_energies) + 1), sorted_energies)\n",
    "\n",
    "    plt.tight_layout()  # Adjust the spacing between subplots\n",
    "    plt.show()\n",
    "\n",
    "# Print the average test losses\n",
    "for i, (mass, _) in enumerate(energies_per_mass, 1):\n",
    "    print(f\"Unique Mass: {mass}, Average Test Loss: {avg_test_losses[i-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea241ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list to store the average test losses for each unique mass\n",
    "avg_test_losses = []\n",
    "\n",
    "# Create an empty list to store the differences for each unique mass\n",
    "differences_per_mass = []\n",
    "\n",
    "# Iterate over the unique masses\n",
    "for i, (mass, energies) in enumerate(energies_per_mass, 1):\n",
    "    # Create a list to store the differences for the current mass\n",
    "    differences = []\n",
    "\n",
    "    # Iterate over the energies for the current mass\n",
    "    for energy in energies:\n",
    "        # Retrieve the data for the current mass and energy\n",
    "        data = df_test[(df_test['A'] == mass) & (df_test['Z'] == energy)]\n",
    "        \n",
    "        # Extract the features (signals) and labels (true values)\n",
    "        signals = data[data.columns[10:-2]].values\n",
    "        signals = Processing[process](signals)\n",
    "        signals = signals[:, :, np.newaxis]\n",
    "        labels1 = data.iloc[:, 9].values\n",
    "        labels2 = data['ToF'].values\n",
    "        labels = labels1 + labels2\n",
    "\n",
    "        # Predict the labels using the model\n",
    "        predicted = model.predict(signals)\n",
    "        predicted = np.squeeze(predicted)\n",
    "\n",
    "        # Calculate the difference between predicted and true labels\n",
    "        difference = predicted - labels\n",
    "        differences.append(difference)  # Store the difference for the current energy\n",
    "\n",
    "    differences_per_mass.append(differences)  # Store the differences for the current mass\n",
    "\n",
    "    # Calculate the average test loss for the current mass\n",
    "    avg_test_loss = np.mean(np.abs(np.concatenate(differences)))  # Concatenate the differences for all energies\n",
    "    avg_test_losses.append(avg_test_loss)  # Store the average test loss\n",
    "\n",
    "# Iterate over the unique masses and corresponding differences\n",
    "for i, (mass, differences) in enumerate(zip(energies_per_mass, differences_per_mass), 1):\n",
    "    energies = mass[1]  # Get the energies for the current mass\n",
    "\n",
    "    # Sort the energies in ascending order\n",
    "    sorted_energies = np.sort(energies)\n",
    "\n",
    "    # Get the indices to reorder the differences based on the sorted energies\n",
    "    indices = np.argsort(energies)\n",
    "    reordered_differences = [differences[index] for index in indices]\n",
    "\n",
    "    plt.figure(figsize=(150, 6))  # Adjust the figure size as needed\n",
    "    plt.subplot(1, len(energies_per_mass), i)  # Create a subplot for each unique mass\n",
    "    plt.boxplot(reordered_differences)\n",
    "    plt.title(f\"Unique Mass: {mass[0]}\")\n",
    "    plt.xlabel(\"Energy\")\n",
    "    plt.ylabel(\"Difference (Predicted - True)\")\n",
    "\n",
    "    # Set the x-axis tick labels to the sorted energies\n",
    "    plt.xticks(range(1, len(sorted_energies) + 1), sorted_energies)\n",
    "\n",
    "    # Add a text box with counts for each energy\n",
    "    counts = [len(d) for d in reordered_differences]\n",
    "    text_box = \"\\n\".join([f\"E{e}: {count}\" for e, count in zip(sorted_energies, counts)])\n",
    "    plt.text(0.95, 0.95, text_box, transform=plt.gca().transAxes, va='top', ha='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()  # Adjust the spacing between subplots\n",
    "    plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bdf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([pd.read_hdf(file_list[i], key=None) for i in list_of_file_ids_test])\n",
    "unique_mass_test = df_test['A'].unique()\n",
    "\n",
    "result_list = []  # List to store results for all unique mass and energy combinations\n",
    "\n",
    "# Test Set\n",
    "for mass_test in unique_mass_test:\n",
    "    df_intermediate = df_test[df_test['A'] == mass_test]\n",
    "    unique_energy = df_intermediate['Z'].unique()\n",
    "\n",
    "    for energy in unique_energy:\n",
    "        df_combination = df_intermediate[df_intermediate['Z'] == energy]\n",
    "\n",
    "        labels1 = df_combination.iloc[:, 9].values\n",
    "        labels2 = df_combination['ToF'].values\n",
    "        labels = labels1 + labels2\n",
    "\n",
    "        signals = df_combination[df_combination.columns[10:-2]].values\n",
    "        signals = Processing[process](signals)\n",
    "        signals = signals[:, :, np.newaxis]\n",
    "\n",
    "        predicted = model.predict(signals)\n",
    "        predicted = np.squeeze(predicted)\n",
    "        true = labels\n",
    "\n",
    "        difference = predicted - true\n",
    "\n",
    "        # Calculate mean and standard deviation\n",
    "        mean = np.mean(difference)\n",
    "        std = np.std(difference)\n",
    "\n",
    "        avg_test_loss = np.mean(np.abs(difference))\n",
    "        count = len(df_combination)\n",
    "\n",
    "        result_list.append([mass_test, energy, avg_test_loss, count, mean, std])\n",
    "\n",
    "# Sort the results by avg_test_loss in ascending order\n",
    "result_list.sort(key=lambda x: x[2])\n",
    "\n",
    "# Print the 5 best cases\n",
    "best_cases = result_list[:5]\n",
    "\n",
    "for case in best_cases:\n",
    "    mass_test, energy, avg_test_loss, count, mean, std = case\n",
    "\n",
    "    # Plotting the distribution\n",
    "    df_combination = df_test[(df_test['A'] == mass_test) & (df_test['Z'] == energy)]\n",
    "    labels1 = df_combination.iloc[:, 9].values\n",
    "    labels2 = df_combination['ToF'].values\n",
    "    labels = labels1 + labels2\n",
    "\n",
    "    signals = df_combination[df_combination.columns[10:-2]].values\n",
    "    signals = Processing[process](signals)\n",
    "    signals = signals[:, :, np.newaxis]\n",
    "\n",
    "    predicted = model.predict(signals)\n",
    "    predicted = np.squeeze(predicted)\n",
    "    true = labels\n",
    "\n",
    "    difference = predicted - true\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(difference, bins=20)\n",
    "    ax.set_title(f\"Best Case - A={mass_test}, Z={energy}, Count={count}\")\n",
    "    ax.set_xlabel(\"Data\")\n",
    "    ax.set_ylabel(\"Difference (predicted - true)\")\n",
    "    ax.text(0.75, 0.9, f\"Mean: {mean:.2f}\\nSTD: {std:.2f}\", transform=ax.transAxes, bbox=dict(facecolor='white'))\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Unique Mass: {mass_test}, Unique Energy: {energy}\")\n",
    "    print(f\"Avg Test Loss: {avg_test_loss}\")\n",
    "    print(f\"Count: {count}\")\n",
    "    print(f\"Mean Test Loss: {mean}\")\n",
    "    print(f\"Standard Deviation Test Loss: {std}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([pd.read_hdf(file_list[i], key=None) for i in list_of_file_ids_test])\n",
    "unique_mass_test = df_test['A'].unique()\n",
    "\n",
    "result_list = []  # List to store results for all unique mass and energy combinations\n",
    "\n",
    "# Test Set\n",
    "for mass_test in unique_mass_test:\n",
    "    df_intermediate = df_test[df_test['A'] == mass_test]\n",
    "    unique_energy = df_intermediate['Z'].unique()\n",
    "\n",
    "    for energy in unique_energy:\n",
    "        df_combination = df_intermediate[df_intermediate['Z'] == energy]\n",
    "\n",
    "        labels1 = df_combination.iloc[:, 9].values\n",
    "        labels2 = df_combination['ToF'].values\n",
    "        labels = labels1 + labels2\n",
    "\n",
    "        signals = df_combination[df_combination.columns[10:-2]].values\n",
    "        signals = Processing[process](signals)\n",
    "        signals = signals[:, :, np.newaxis]\n",
    "\n",
    "        predicted = model.predict(signals)\n",
    "        predicted = np.squeeze(predicted)\n",
    "        true = labels\n",
    "\n",
    "        difference = predicted - true\n",
    "\n",
    "        # Calculate mean and standard deviation\n",
    "        mean = np.mean(difference)\n",
    "        std = np.std(difference)\n",
    "\n",
    "        avg_test_loss = np.mean(np.abs(difference))\n",
    "        count = len(df_combination)\n",
    "\n",
    "        result_list.append([mass_test, energy, avg_test_loss, count, mean, std])\n",
    "\n",
    "# Sort the results by avg_test_loss in ascending order\n",
    "result_list.sort(key=lambda x: x[2])\n",
    "\n",
    "# Print the 5 worst cases\n",
    "worst_cases = result_list[-5:]\n",
    "\n",
    "for case in worst_cases:\n",
    "    mass_test, energy, avg_test_loss, count, mean, std = case\n",
    "\n",
    "    # Plotting the distribution\n",
    "    df_combination = df_test[(df_test['A'] == mass_test) & (df_test['Z'] == energy)]\n",
    "    labels1 = df_combination.iloc[:, 9].values\n",
    "    labels2 = df_combination['ToF'].values\n",
    "    labels = labels1 + labels2\n",
    "\n",
    "    signals = df_combination[df_combination.columns[10:-2]].values\n",
    "    signals = Processing[process](signals)\n",
    "    signals = signals[:, :, np.newaxis]\n",
    "\n",
    "    predicted = model.predict(signals)\n",
    "    predicted = np.squeeze(predicted)\n",
    "    true = labels\n",
    "\n",
    "    difference = predicted - true\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(difference, bins=20)\n",
    "    ax.set_title(f\"Worst Case - A={mass_test}, Z={energy}, Count={count}\")\n",
    "    ax.set_xlabel(\"Data\")\n",
    "    ax.set_ylabel(\"Difference (predicted - true)\")\n",
    "    ax.text(0.75, 0.9, f\"Mean: {mean:.2f}\\nSTD: {std:.2f}\", transform=ax.transAxes, bbox=dict(facecolor='white'))\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Unique Mass: {mass_test}, Unique Energy: {energy}\")\n",
    "    print(f\"Avg Test Loss: {avg_test_loss}\")\n",
    "    print(f\"Count: {count}\")\n",
    "    print(f\"Mean Test Loss: {mean}\")\n",
    "    print(f\"Standard Deviation Test Loss: {std}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f309c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the test files into a single DataFrame\n",
    "df_test = pd.concat([pd.read_hdf(file_list[file_id], key=None) for file_id in list_of_file_ids_test])\n",
    "\n",
    "labels1 = df_test.iloc[:, 9].values\n",
    "labels2 = df_test['ToF'].values\n",
    "labels = labels1 + labels2\n",
    "\n",
    "signals = df_test[df_test.columns[10:-2]].values\n",
    "signals = Processing[process](signals)\n",
    "signals = signals[:, :, np.newaxis]\n",
    "\n",
    "# model.predict\n",
    "predicted = model.predict(signals)\n",
    "predicted = np.squeeze(predicted)  # getting rid of extra dimension\n",
    "true = labels\n",
    "\n",
    "difference = predicted - true\n",
    "# Your code to generate the histogram\n",
    "plt.hist(difference, bins=100)\n",
    "plt.title(\"Difference between Predicted and True Values\")\n",
    "plt.xlabel(\"Difference (Predicted - True) (ns)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean = np.mean(difference)\n",
    "std = np.std(difference)\n",
    "\n",
    "# Create legend text with mean and standard deviation\n",
    "legend_text = f\"Mean: {mean:.2f}\\nSTD: {std:.2f}\"\n",
    "\n",
    "# Add legend with the mean and standard deviation\n",
    "plt.legend([legend_text])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the test files into a single DataFrame\n",
    "df_test = pd.concat([pd.read_hdf(file_list[file_id], key=None) for file_id in list_of_file_ids_test])\n",
    "\n",
    "labels1 = df_test.iloc[:, 9].values\n",
    "labels2 = df_test['ToF'].values\n",
    "labels = labels1 + labels2\n",
    "\n",
    "signals = df_test[df_test.columns[10:-2]].values\n",
    "signals = Processing[process](signals)\n",
    "signals = signals[:, :, np.newaxis]\n",
    "\n",
    "# model.predict\n",
    "predicted = model.predict(signals)\n",
    "predicted = np.squeeze(predicted)  # getting rid of extra dimension\n",
    "true = labels\n",
    "\n",
    "difference = predicted - true\n",
    "# Plotting the histogram\n",
    "plt.hist(difference, bins=100)\n",
    "plt.title(\"Difference between Predicted and True Values\")\n",
    "plt.xlabel(\"Difference (Predicted - True) (ns)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.yscale('log')  # Set y-axis to logarithmic scale\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean = np.mean(difference)\n",
    "std = np.std(difference)\n",
    "\n",
    "# Create legend text with mean and standard deviation\n",
    "legend_text = f\"Mean: {mean:.2f}\\nSTD: {std:.2f}\"\n",
    "\n",
    "# Add legend with the mean and standard deviation\n",
    "plt.legend([legend_text])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
