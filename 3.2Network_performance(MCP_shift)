{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1909a68",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3264\\3112716107.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import struct\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc06c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import struct\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd7c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = pathlib.Path(\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Data/new_pdt/Processed/\")\n",
    "\n",
    "file_list = list(direction.iterdir())\n",
    "\n",
    "file_num = len(file_list)\n",
    "\n",
    "num_test_files = 2\n",
    "eighty = round(0.8*file_num)\n",
    "twenty = file_num-eighty - num_test_files\n",
    "\n",
    "list_of_file_ids_train = np.arange(eighty, dtype=int)\n",
    "\n",
    "list_of_file_ids_val = np.arange(eighty,eighty+twenty-num_test_files, dtype=int)\n",
    "\n",
    "list_of_file_ids_test =np.arange(file_num-num_test_files,file_num)\n",
    "\n",
    "\n",
    "##### Functions to process the data\n",
    "\n",
    "        ### Unnormalization of each signal individually\n",
    "def Unnormalized(batch_signals):\n",
    "        \n",
    "        return batch_signals\n",
    "        \n",
    "        ### Normalization of each signal individually\n",
    "def Normalized(batch_signals):\n",
    "\n",
    "        for i in range(len(batch_signals)):\n",
    "            batch_signals[i] = batch_signals[i]/np.max(batch_signals[i])\n",
    "            \n",
    "        return batch_signals\n",
    "            \n",
    "        \n",
    "        ### Normalization of the entire value by one common denominator      \n",
    "def Denominator(batch_signals):  \n",
    "    \n",
    "        denominator = 3953.48\n",
    "        batch_signals = batch_signals/denominator\n",
    "        \n",
    "        return batch_signals\n",
    "\n",
    "\n",
    "##### Class\n",
    "\n",
    "class TrainDataset(tf.data.Dataset):\n",
    "\n",
    "    def _generator(file_id):  \n",
    "#         print(f'Using Train Class')\n",
    "        if(file_id == 0):\n",
    "#             print(\"reshuffling\")\n",
    "            np.random.shuffle(list_of_file_ids_train)             \n",
    "\n",
    "        i_file = list_of_file_ids_train[file_id]\n",
    "\n",
    "#         print(f'file_id: {file_id}, i_file: {i_file}')\n",
    "#         print()\n",
    "        signal_filename = direction/f'{i_file+1}.h5'\n",
    "\n",
    "        \n",
    "         # Load the labels and signals from the files\n",
    "        df = pd.read_hdf(signal_filename,key=None)  \n",
    "        \n",
    "        labels1 = df.iloc[:,9].values\n",
    "        labels2 = df['ToF'].values\n",
    "        shift = df.iloc[:-1].values\n",
    "        labels = labels1+labels2+shift\n",
    "        \n",
    "        signals = df[df.columns[10+10-shift:-2-10-shift]].values\n",
    "        \n",
    "        \n",
    "        \n",
    "        length = signals.shape[1]\n",
    "        \n",
    "        # Determine how many batches can be made from this file\n",
    "        num_batches = len(signals) // batch_size\n",
    "\n",
    "        # Shuffle the signals within the file\n",
    "        signal_indices = np.arange(len(signals))\n",
    "        np.random.shuffle(signal_indices)        \n",
    "        \n",
    "        # Loop through each batch in the file\n",
    "        for batch_idx in range(num_batches):\n",
    "            # Get the signals and labels for this batch\n",
    "            batch_signal_indices = signal_indices[batch_idx*batch_size:(batch_idx+1)*batch_size]      \n",
    " \n",
    "            batch_signals = signals[batch_signal_indices]\n",
    "            \n",
    "            batch_signals = Processing[process](batch_signals)\n",
    "                \n",
    "            batch_signals = batch_signals[:,:,np.newaxis] # Can also be done with signals = signals[:,:,np.newaxis]\n",
    "            batch_labels = labels[batch_signal_indices]\n",
    "\n",
    "            # Yield the signals and labels as a tuple\n",
    "            yield batch_signals, batch_labels \n",
    "             \n",
    "    def __new__(cls, file_id):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=(tf.dtypes.float64, tf.dtypes.float64),\n",
    "            output_shapes=((batch_size, length,1), (batch_size, )),\n",
    "            args=(file_id,)\n",
    "        )\n",
    "    \n",
    "class ValDataset(tf.data.Dataset):\n",
    "\n",
    "    def _generator(file_id):  \n",
    "#         print(f'Using Val Class')\n",
    "        i_file = list_of_file_ids_val[file_id]\n",
    "    \n",
    "        signal_filename = direction/f'{i_file+1}.h5'\n",
    "\n",
    "         # Load the labels and signals from the files\n",
    "        df = pd.read_hdf(signal_filename,key=None)    \n",
    "        \n",
    "         # Load the labels and signals from the files\n",
    "        df = pd.read_hdf(signal_filename,key=None)  \n",
    "        \n",
    "        labels1 = df.iloc[:,9].values\n",
    "        labels2 = df['ToF'].values\n",
    "        shift = df.iloc[:-1].values\n",
    "        labels = labels1+labels2+shift\n",
    "        \n",
    "        signals = df[df.columns[10+10-shift:-2-10-shift]].values\n",
    "        \n",
    "        length = signals.shape[1]\n",
    "        \n",
    "        # Determine how many batches can be made from this file\n",
    "        num_batches = len(signals) // batch_size\n",
    "\n",
    "        # Shuffle the signals within the file\n",
    "        signal_indices = np.arange(len(signals))\n",
    "        np.random.shuffle(signal_indices)        \n",
    "        \n",
    "        # Loop through each batch in the file\n",
    "        for batch_idx in range(num_batches):\n",
    "            # Get the signals and labels for this batch\n",
    "            batch_signal_indices = signal_indices[batch_idx*batch_size:(batch_idx+1)*batch_size]      \n",
    " \n",
    "            batch_signals = signals[batch_signal_indices]\n",
    "            \n",
    "            batch_signals = Processing[process](batch_signals)\n",
    "                \n",
    "            batch_signals = batch_signals[:,:,np.newaxis] # Can also be done with signals = signals[:,:,np.newaxis]\n",
    "            batch_labels = labels[batch_signal_indices]\n",
    "\n",
    "            # Yield the signals and labels as a tuple\n",
    "            yield batch_signals, batch_labels\n",
    "             \n",
    "    def __new__(cls, file_id):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=(tf.dtypes.float64, tf.dtypes.float64),\n",
    "            output_shapes=((batch_size, length,1), (batch_size, )),\n",
    "            args=(file_id,)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5161be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(Conv1D(filters=16, kernel_size=5,activation='relu', input_shape=(1998, 1)))\n",
    "model.add(Conv1D(filters=8, kernel_size=5,dilation_rate=2,activation='relu'))\n",
    "model.add(Conv1D(filters=4, kernel_size=5,dilation_rate=2,activation='relu'))\n",
    "model.add(Conv1D(filters=4, kernel_size=5,strides=2,activation='relu'))\n",
    "model.add(Conv1D(filters=4, kernel_size=3,strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80086845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing options\n",
    "Processing = [Unnormalized,Normalized,Denominator]\n",
    "process = 2\n",
    "\n",
    "# Training Variables\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "steps_per_epoch = eighty*5000 // batch_size\n",
    "\n",
    "initial_lr = 1e-03\n",
    "final_lr = 1e-06\n",
    "\n",
    "# initial_lr = 1e-04\n",
    "# final_lr = 1e-04\n",
    "\n",
    "def step_decay(epoch):\n",
    "    lrate = initial_lr * (final_lr/initial_lr)**(epoch/num_epochs)\n",
    "\n",
    "    print(f'Current Learning rate: {lrate}')\n",
    "    return lrate\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_absolute_error', optimizer = keras.optimizers.Adam(initial_lr), metrics=['mean_absolute_error','mean_squared_error'])\n",
    "\n",
    "# Configuring training dataset\n",
    "dataset_train = tf.data.Dataset.range(eighty).interleave(\n",
    "        TrainDataset,\n",
    "        cycle_length=2,\n",
    "        num_parallel_calls=2,\n",
    "        deterministic=True).repeat().prefetch(1)\n",
    "\n",
    "\n",
    "\n",
    "# Configuring training dataset\n",
    "dataset_val = tf.data.Dataset.range(twenty-num_test_files).interleave(\n",
    "        ValDataset,\n",
    "        cycle_length=2,\n",
    "        num_parallel_calls=2,\n",
    "        deterministic=True).prefetch(1)\n",
    "\n",
    "\n",
    "# Callback Functions\n",
    "LRS = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "ES = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True,verbose=1)\n",
    "\n",
    "CSV = tf.keras.callbacks.CSVLogger(\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Data/Log.csv\",\n",
    "                                   separator=\",\", append=True)\n",
    "\n",
    "callbacks = [ES,LRS,CSV]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9095a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=dataset_train, validation_data = dataset_val, steps_per_epoch=steps_per_epoch, epochs=num_epochs,callbacks=callbacks)\n",
    "\n",
    "# lossthing = []\n",
    "\n",
    "# for j in range(3):\n",
    "#     process = j\n",
    "#     for i in range(3):\n",
    "#         history = model.fit(x=dataset_train, validation_data = dataset_val, steps_per_epoch=steps_per_epoch, epochs=num_epochs,callbacks=callbacks)\n",
    "#         lossthing.append(history.history['val_loss'][-1])\n",
    "            \n",
    "# print(lossthing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6964890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### All Analysis Tools:::\n",
    "\n",
    "# Loss/Val vs Epochs\n",
    "loss = history.history['loss']\n",
    "val = history.history['val_loss']\n",
    "\n",
    "x1 = np.arange(1,len(loss))\n",
    "x2 = np.arange(2,len(loss))\n",
    "\n",
    "# print(f'Training: {loss} \\n')\n",
    "# print(f'Validation: {val}')\n",
    "\n",
    "print(f'Train Lowest: {loss[-1]}')\n",
    "print(f'Val Lowest: {val[-1]}')\n",
    "\n",
    "# Full Training\n",
    "plt.plot(loss,label='Train')\n",
    "plt.title(f' Model Performance (Variable: PDT)')\n",
    "plt.plot(val,label='Validation')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Full training without first epoch\n",
    "plt.plot(x1,loss[1:],label='Train')\n",
    "plt.plot(x1,val[1:],label='Validation')\n",
    "plt.title(f' Model Performance (Variable: PDT)')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Full training without first two epochs\n",
    "plt.plot(x2,loss[2:],label='Train')\n",
    "plt.plot(x2,val[2:],label='Validation')\n",
    "plt.title(f' Model Performance (Variable: PDT)')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941aa477",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = []\n",
    "\n",
    "\n",
    "for i in list_of_file_ids_test:\n",
    "    df = pd.read_hdf(file_list[i],key=None)   \n",
    "\n",
    "    labels1 = df.iloc[:,9].values\n",
    "    labels2 = df['ToF'].values\n",
    "    labels = labels1+labels2\n",
    "\n",
    "    signals = df[df.columns[10:-2]].values\n",
    "    signals = Processing[process](signals)\n",
    "    signals = signals[:,:,np.newaxis]\n",
    "\n",
    "    # model.predict\n",
    "    predicted = model.predict(signals)\n",
    "    predicted = np.squeeze(predicted) # getting rid of extra dimension\n",
    "    true = labels\n",
    "    \n",
    "    test_loss = np.subtract(predicted,true)\n",
    "    \n",
    "    avg_test_loss = np.mean(np.abs(test_loss))\n",
    "    print(avg_test_loss)\n",
    "    avg_loss.append(avg_test_loss)\n",
    "    \n",
    "print(f'Mean Absolute Error: {np.mean(avg_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = []\n",
    "\n",
    "for i in list_of_file_ids_test:\n",
    "    df = pd.read_hdf(file_list[i], key=None)\n",
    "    \n",
    "    labels1 = df.iloc[:, 9].values\n",
    "    labels2 = df['ToF'].values\n",
    "    labels = labels1 + labels2\n",
    "    \n",
    "    signals = df[df.columns[10:-2]].values\n",
    "    signals = Processing[process](signals)\n",
    "    signals = signals[:, :, np.newaxis]\n",
    "    \n",
    "    # model.predict\n",
    "    predicted = model.predict(signals)\n",
    "    predicted = np.squeeze(predicted)  # getting rid of extra dimension\n",
    "    true = labels\n",
    "    \n",
    "    test_loss = np.subtract(predicted, true)\n",
    "    \n",
    "    # Calculate mean squared error (MSE)\n",
    "    mse = np.mean(np.square(test_loss))\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    \n",
    "    avg_loss.append(mse)\n",
    "    \n",
    "print(f'Mean Squared Error: {np.mean(avg_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b126bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "direction = pathlib.Path(\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Data/new_pdt/Processed/\")\n",
    "file_list = list(direction.iterdir())\n",
    "\n",
    "df = pd.read_hdf(file_list[0], key=None)\n",
    "unique_mass = df['A'].unique()\n",
    "\n",
    "result_dict = {}  # Dictionary to store unique_mass and corresponding avg_test_loss values\n",
    "\n",
    "for i in range(len(unique_mass)):\n",
    "    df_intermediate = df[df['A'] == unique_mass[i]]\n",
    "    unique_energy = df_intermediate['Z'].unique()\n",
    "\n",
    "    avg_test_loss_list = []  # List to store avg_test_loss values for each unique_mass\n",
    "\n",
    "    for j in range(len(unique_energy)):\n",
    "        mass_u = unique_mass[i]\n",
    "        energy_u = unique_energy[j]\n",
    "        print(f\"Unique Mass: {mass_u}, Unique Energy: {energy_u}\")\n",
    "        loss = []\n",
    "        for file in file_list:\n",
    "            df = pd.read_hdf(file, key=None)\n",
    "\n",
    "            # Step 1: Apply the first condition and create an intermediate DataFrame\n",
    "            df_intermediate = df[df['A'] == mass_u]\n",
    "\n",
    "            # Step 2: Apply the second condition to the intermediate DataFrame and create the final filtered DataFrame\n",
    "            df_filtered = df_intermediate[df_intermediate['Z'] == energy_u]\n",
    "\n",
    "            # preprocessing the input data\n",
    "            signals = df_filtered[df_filtered.columns[10:-2]].values\n",
    "            signals = Processing[process](signals)\n",
    "            signals = signals[:, :, np.newaxis]\n",
    "\n",
    "            # model.predict\n",
    "            predicted = model.predict(signals)\n",
    "            predicted = np.squeeze(predicted)  # getting rid of extra dimension\n",
    "\n",
    "            labels1 = df_filtered.iloc[:, 9].values\n",
    "            labels2 = df_filtered['ToF'].values\n",
    "            true = labels1 + labels2\n",
    "\n",
    "            test_loss = np.subtract(predicted, true)\n",
    "\n",
    "            avg_test_loss = np.mean(np.abs(test_loss))\n",
    "\n",
    "            loss.append(avg_test_loss)\n",
    "\n",
    "        avg_test_loss_list.append(np.mean(loss))\n",
    "\n",
    "    result_dict[unique_mass[i]] = {\n",
    "        'unique_energy': unique_energy,\n",
    "        'avg_test_loss': avg_test_loss_list\n",
    "    }\n",
    "\n",
    "# Print the result dictionary\n",
    "for mass, result in result_dict.items():\n",
    "    print(f\"Unique Mass: {mass}\")\n",
    "    unique_energy = result['unique_energy']\n",
    "    avg_test_loss_list = result['avg_test_loss']\n",
    "    for energy, avg_test_loss in zip(unique_energy, avg_test_loss_list):\n",
    "        print(f\"Unique Energy: {energy}\")\n",
    "        print(f\"Avg Test Loss: {avg_test_loss}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
