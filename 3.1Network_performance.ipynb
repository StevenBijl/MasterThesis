{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fdcbe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with Deep Learning\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting with Deep Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff01ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import struct\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tabulate import tabulate\n",
    "import csv\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871749f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import struct\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tabulate import tabulate\n",
    "import csv\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae38eb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5774bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "275424f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31]\n",
      "[32 33 34 35]\n",
      "[38 39]\n"
     ]
    }
   ],
   "source": [
    "### Usable Files\n",
    "\n",
    "direction = pathlib.Path(\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Data/Detector97/Filtered/Processed/\")\n",
    "\n",
    "# direction = pathlib.Path(\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Data/Detector97/Unfiltered/Processed/\")\n",
    "\n",
    "file_list = list(direction.iterdir())\n",
    "\n",
    "file_num = len(file_list)\n",
    "\n",
    "step = 0.8\n",
    "\n",
    "num_test_files = 2\n",
    "eighty = round(step*file_num)\n",
    "twenty = file_num-eighty - num_test_files\n",
    "\n",
    "list_of_file_ids_train = np.arange(eighty, dtype=int)\n",
    "print(list_of_file_ids_train)\n",
    "list_of_file_ids_val = np.arange(eighty,eighty+twenty-num_test_files, dtype=int)\n",
    "print(list_of_file_ids_val)\n",
    "list_of_file_ids_test =np.arange(file_num-num_test_files,file_num)\n",
    "print(list_of_file_ids_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef1a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functions to process the data\n",
    "\n",
    "        ### Unnormalization of each signal individually\n",
    "def Unnormalized(batch_signals):\n",
    "        \n",
    "        return batch_signals\n",
    "        \n",
    "        ### Normalization of each signal individually\n",
    "def Normalized(batch_signals):\n",
    "\n",
    "        for i in range(len(batch_signals)):\n",
    "            batch_signals[i] = batch_signals[i]/np.max(batch_signals[i])\n",
    "            \n",
    "        return batch_signals\n",
    "            \n",
    "        \n",
    "        ### Normalization of the entire value by one common denominator      \n",
    "def Denominator(batch_signals):  \n",
    "    \n",
    "        denominator = 3953.48\n",
    "        batch_signals = batch_signals/denominator\n",
    "        \n",
    "        return batch_signals\n",
    "\n",
    "\n",
    "##### Class\n",
    "\n",
    "class TrainDataset(tf.data.Dataset):\n",
    "\n",
    "    def _generator(file_id):  \n",
    "#         print(f'Using Train Class')\n",
    "        if(file_id == 0):\n",
    "#             print(\"reshuffling\")\n",
    "            np.random.shuffle(list_of_file_ids_train)             \n",
    "\n",
    "        i_file = list_of_file_ids_train[file_id]\n",
    "\n",
    "#         print(f'file_id: {file_id}, i_file: {i_file}')\n",
    "#         print()\n",
    "        signal_filename = direction/f'{i_file+1}.h5'\n",
    "\n",
    "        \n",
    "         # Load the labels and signals from the files\n",
    "        df = pd.read_hdf(signal_filename,key=None)  \n",
    "        \n",
    "        labels1 = df.iloc[:,9].values\n",
    "        labels2 = df['ToF'].values\n",
    "        labels = labels1+labels2\n",
    "        \n",
    "        signals = df[df.columns[10:-2]].values\n",
    "        \n",
    "        \n",
    "        # Determine how many batches can be made from this file\n",
    "        num_batches = len(signals) // batch_size\n",
    "\n",
    "        # Shuffle the signals within the file\n",
    "        signal_indices = np.arange(len(signals))\n",
    "        np.random.shuffle(signal_indices)        \n",
    "        \n",
    "        # Loop through each batch in the file\n",
    "        for batch_idx in range(num_batches):\n",
    "            # Get the signals and labels for this batch\n",
    "            batch_signal_indices = signal_indices[batch_idx*batch_size:(batch_idx+1)*batch_size]      \n",
    " \n",
    "            batch_signals = signals[batch_signal_indices]\n",
    "            \n",
    "            batch_signals = Processing[process](batch_signals)\n",
    "                \n",
    "            batch_signals = batch_signals[:,:,np.newaxis] # Can also be done with signals = signals[:,:,np.newaxis]\n",
    "            batch_labels = labels[batch_signal_indices]\n",
    "\n",
    "            # Yield the signals and labels as a tuple\n",
    "            yield batch_signals, batch_labels \n",
    "             \n",
    "    def __new__(cls, file_id):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=(tf.dtypes.float64, tf.dtypes.float64),\n",
    "            output_shapes=((batch_size, 1998,1), (batch_size, )),\n",
    "            args=(file_id,)\n",
    "        )\n",
    "    \n",
    "class ValDataset(tf.data.Dataset):\n",
    "\n",
    "    def _generator(file_id):  \n",
    "#         print(f'Using Val Class')\n",
    "        i_file = list_of_file_ids_val[file_id]\n",
    "    \n",
    "        signal_filename = direction/f'{i_file+1}.h5'\n",
    "\n",
    "         # Load the labels and signals from the files\n",
    "        df = pd.read_hdf(signal_filename,key=None)    \n",
    "        \n",
    "        labels1 = df.iloc[:,9].values\n",
    "        labels2 = df['ToF'].values\n",
    "        labels = labels1+labels2\n",
    "        \n",
    "        signals = df[df.columns[10:-2]].values\n",
    "        \n",
    "        \n",
    "        # Determine how many batches can be made from this file\n",
    "        num_batches = len(signals) // batch_size\n",
    "\n",
    "        # Shuffle the signals within the file\n",
    "        signal_indices = np.arange(len(signals))\n",
    "        np.random.shuffle(signal_indices)        \n",
    "        \n",
    "        # Loop through each batch in the file\n",
    "        for batch_idx in range(num_batches):\n",
    "            # Get the signals and labels for this batch\n",
    "            batch_signal_indices = signal_indices[batch_idx*batch_size:(batch_idx+1)*batch_size]      \n",
    " \n",
    "            batch_signals = signals[batch_signal_indices]\n",
    "            \n",
    "            batch_signals = Processing[process](batch_signals)\n",
    "                \n",
    "            batch_signals = batch_signals[:,:,np.newaxis] # Can also be done with signals = signals[:,:,np.newaxis]\n",
    "            batch_labels = labels[batch_signal_indices]\n",
    "\n",
    "            # Yield the signals and labels as a tuple\n",
    "            yield batch_signals, batch_labels\n",
    "             \n",
    "    def __new__(cls, file_id):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=(tf.dtypes.float64, tf.dtypes.float64),\n",
    "            output_shapes=((batch_size, 1998,1), (batch_size, )),\n",
    "            args=(file_id,)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e9da4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499.14654541 499.83734131 499.82849121 ... 499.1619873  499.05609131\n",
      " 499.6555481 ]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_hdf(file_list[0],key=None)  \n",
    "\n",
    "labels1 = df.iloc[:,9].values\n",
    "labels2 = df['ToF'].values\n",
    "labels = labels1+labels2\n",
    "\n",
    "print(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c979446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential()\n",
    "# model.add(Conv1D(filters=16, kernel_size=5,activation='relu', input_shape=(1998, 1)))\n",
    "# model.add(Conv1D(filters=8, kernel_size=5,dilation_rate=2,activation='relu'))\n",
    "# model.add(Conv1D(filters=4, kernel_size=5,dilation_rate=2,activation='relu'))\n",
    "# model.add(Conv1D(filters=4, kernel_size=5,strides=2,activation='relu'))\n",
    "# model.add(Conv1D(filters=4, kernel_size=3,strides=2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(32,activation='relu'))\n",
    "# model.add(Dense(16,activation='relu'))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92e20fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv1D(filters=16, kernel_size=5, activation='relu', input_shape=(1998, 1)))\n",
    "    model.add(Conv1D(filters=8, kernel_size=5, dilation_rate=2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=4, kernel_size=5, dilation_rate=2, activation='relu'))\n",
    "    model.add(Conv1D(filters=4, kernel_size=5, strides=2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=4, kernel_size=3, strides=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f780b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "# # Define the regularization parameters\n",
    "# l1 = 0.001  # L1 regularization parameter\n",
    "# l2 = 0.001  # L2 regularization parameter\n",
    "\n",
    "# # Create the model\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(filters=16, kernel_size=5, activation='relu', input_shape=(1998, 1), kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n",
    "# model.add(Conv1D(filters=8, kernel_size=5, dilation_rate=2, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv1D(filters=4, kernel_size=5, dilation_rate=2, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n",
    "# model.add(Conv1D(filters=4, kernel_size=5, strides=2, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv1D(filters=4, kernel_size=3, strides=2, kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n",
    "# model.add(Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=l1, l2=l2)))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff5031de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential()\n",
    "# model.add(Conv1D(filters=16, kernel_size=5,activation=tf.keras.layers.LeakyReLU(alpha=0.01), input_shape=(1998, 1)))\n",
    "# model.add(Conv1D(filters=8, kernel_size=5,dilation_rate=2,activation=tf.keras.layers.LeakyReLU(alpha=0.01)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv1D(filters=4, kernel_size=5,dilation_rate=2,activation=tf.keras.layers.LeakyReLU(alpha=0.01)))\n",
    "# model.add(Conv1D(filters=4, kernel_size=5,strides=2,activation=tf.keras.layers.LeakyReLU(alpha=0.01)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv1D(filters=4, kernel_size=3,strides=2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(32,activation=tf.keras.layers.LeakyReLU(alpha=0.01)))\n",
    "# model.add(Dense(16,activation=tf.keras.layers.LeakyReLU(alpha=0.01)))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f2aea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv1D, BatchNormalization, Flatten, Dense, MaxPooling1D, Add\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(filters=16, kernel_size=5, activation='relu', input_shape=(1998, 1)))\n",
    "# model.add(Conv1D(filters=16, kernel_size=5, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Conv1D(filters=32, kernel_size=5, dilation_rate=2, activation='relu'))\n",
    "# model.add(Conv1D(filters=32, kernel_size=5, dilation_rate=2, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Conv1D(filters=64, kernel_size=5, dilation_rate=2, activation='relu'))\n",
    "# model.add(Conv1D(filters=64, kernel_size=5, dilation_rate=2, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Conv1D(filters=128, kernel_size=5, dilation_rate=2, activation='relu'))\n",
    "# model.add(Conv1D(filters=128, kernel_size=5, dilation_rate=2, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89a576a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_2800\\2373516906.py:76: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_2800\\2373516906.py:76: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing options\n",
    "Processing = {\n",
    "    \"Unnormalized\": Unnormalized,\n",
    "    \"Normalized\": Normalized,\n",
    "    \"Denominator\": Denominator\n",
    "}\n",
    "Process = [\"Unnormalized\",\"Normalized\",\"Denominator\"]\n",
    "process = Process[2]\n",
    "# Loss Function\n",
    "\n",
    "loss_function = ['mean_absolute_error','mean_squared_error']\n",
    "lf = 1\n",
    "\n",
    "# Training Variables\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "steps_per_epoch = eighty*5000 // batch_size\n",
    "\n",
    "# Learning Rate\n",
    "initial_lr = 1e-03\n",
    "final_lr = 1e-06\n",
    "\n",
    "# initial_lr = 1e-03\n",
    "# final_lr = 1e-03\n",
    "\n",
    "def step_decay(epoch):\n",
    "    lrate = initial_lr * (final_lr/initial_lr)**(epoch/num_epochs)\n",
    "\n",
    "    print(f'Current Learning rate: {lrate}')\n",
    "    return lrate\n",
    "\n",
    "# Compile the model\n",
    "# model.compile(loss=loss_function[lf], optimizer = keras.optimizers.Adam(initial_lr), metrics=['mean_absolute_error','mean_squared_error'])\n",
    "\n",
    "# Configuring training dataset\n",
    "dataset_train = tf.data.Dataset.range(eighty).interleave(\n",
    "        TrainDataset,\n",
    "        cycle_length=2,\n",
    "        num_parallel_calls=2,\n",
    "        deterministic=True).repeat().prefetch(1)\n",
    "\n",
    "\n",
    "\n",
    "# Configuring training dataset\n",
    "dataset_val = tf.data.Dataset.range(twenty-num_test_files).interleave(\n",
    "        ValDataset,\n",
    "        cycle_length=2,\n",
    "        num_parallel_calls=2,\n",
    "        deterministic=True).prefetch(1)\n",
    "\n",
    "\n",
    "# Callback Functions\n",
    "LRS = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "ES = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True,verbose=1)\n",
    "\n",
    "CSV = tf.keras.callbacks.CSVLogger(\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Data/Log.csv\",\n",
    "                                separator=\",\", append=True)\n",
    "\n",
    "MC_path = f\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Trainings/model_checkpoint.h5\"\n",
    "MC = ModelCheckpoint(\n",
    "    filepath=MC_path,  # Filepath to save the model weights\n",
    "    monitor='val_loss',  # Quantity to monitor (e.g., validation loss)\n",
    "    save_best_only=True,  # Save only the best model based on the monitored quantity\n",
    "    save_weights_only=True  # Save only the model weights, not the entire model\n",
    ")\n",
    "\n",
    "callbacks = [MC,LRS,CSV]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a103f83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = create_model()\n",
    "# model.compile(loss=loss_function[lf], optimizer = keras.optimizers.Adam(initial_lr), metrics=['mean_absolute_error','mean_squared_error'])\n",
    "# history = model.fit(x=dataset_train, validation_data = dataset_val, steps_per_epoch=steps_per_epoch, epochs=num_epochs,callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf232d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/3\n",
      "Epoch 1/30\n",
      "Current Learning rate: 0.001\n",
      "5000/5000 [==============================] - 59s 8ms/step - loss: 1923.9802 - mean_absolute_error: 5.5665 - mean_squared_error: 1923.9802 - val_loss: 0.6474 - val_mean_absolute_error: 0.7117 - val_mean_squared_error: 0.6474\n",
      "Epoch 2/30\n",
      "Current Learning rate: 0.0007943282347242815\n",
      "5000/5000 [==============================] - 38s 8ms/step - loss: 0.3923 - mean_absolute_error: 0.4845 - mean_squared_error: 0.3923 - val_loss: 1.0234 - val_mean_absolute_error: 0.9716 - val_mean_squared_error: 1.0234\n",
      "Epoch 3/30\n",
      "Current Learning rate: 0.0006309573444801933\n",
      "5000/5000 [==============================] - 37s 7ms/step - loss: 0.4097 - mean_absolute_error: 0.4083 - mean_squared_error: 0.4097 - val_loss: 45.3878 - val_mean_absolute_error: 6.7220 - val_mean_squared_error: 45.3878\n",
      "Epoch 4/30\n",
      "Current Learning rate: 0.0005011872336272722\n",
      "5000/5000 [==============================] - 34s 7ms/step - loss: 0.1343 - mean_absolute_error: 0.2850 - mean_squared_error: 0.1343 - val_loss: 3.3990 - val_mean_absolute_error: 1.8375 - val_mean_squared_error: 3.3990\n",
      "Epoch 5/30\n",
      "Current Learning rate: 0.00039810717055349724\n",
      "5000/5000 [==============================] - 38s 8ms/step - loss: 0.1823 - mean_absolute_error: 0.3131 - mean_squared_error: 0.1823 - val_loss: 3.9251 - val_mean_absolute_error: 1.9701 - val_mean_squared_error: 3.9251\n",
      "Epoch 6/30\n",
      "Current Learning rate: 0.00031622776601683794\n",
      "5000/5000 [==============================] - 35s 7ms/step - loss: 0.0919 - mean_absolute_error: 0.2376 - mean_squared_error: 0.0919 - val_loss: 1.6388 - val_mean_absolute_error: 1.2691 - val_mean_squared_error: 1.6388\n",
      "Epoch 7/30\n",
      "Current Learning rate: 0.000251188643150958\n",
      "5000/5000 [==============================] - 47s 9ms/step - loss: 0.0687 - mean_absolute_error: 0.2065 - mean_squared_error: 0.0687 - val_loss: 0.1312 - val_mean_absolute_error: 0.3266 - val_mean_squared_error: 0.1312\n",
      "Epoch 8/30\n",
      "Current Learning rate: 0.00019952623149688796\n",
      "5000/5000 [==============================] - 47s 9ms/step - loss: 0.0574 - mean_absolute_error: 0.1884 - mean_squared_error: 0.0574 - val_loss: 0.1689 - val_mean_absolute_error: 0.3808 - val_mean_squared_error: 0.1689\n",
      "Epoch 9/30\n",
      "Current Learning rate: 0.00015848931924611134\n",
      "5000/5000 [==============================] - 63s 13ms/step - loss: 0.0476 - mean_absolute_error: 0.1713 - mean_squared_error: 0.0476 - val_loss: 2.5002 - val_mean_absolute_error: 1.5718 - val_mean_squared_error: 2.5002\n",
      "Epoch 10/30\n",
      "Current Learning rate: 0.00012589254117941674\n",
      "5000/5000 [==============================] - 50s 10ms/step - loss: 0.0403 - mean_absolute_error: 0.1566 - mean_squared_error: 0.0403 - val_loss: 0.5655 - val_mean_absolute_error: 0.7384 - val_mean_squared_error: 0.5655\n",
      "Epoch 11/30\n",
      "Current Learning rate: 0.00010000000000000002\n",
      "5000/5000 [==============================] - 63s 13ms/step - loss: 0.0359 - mean_absolute_error: 0.1487 - mean_squared_error: 0.0359 - val_loss: 0.0485 - val_mean_absolute_error: 0.1843 - val_mean_squared_error: 0.0485\n",
      "Epoch 12/30\n",
      "Current Learning rate: 7.943282347242817e-05\n",
      "5000/5000 [==============================] - 56s 11ms/step - loss: 0.0337 - mean_absolute_error: 0.1435 - mean_squared_error: 0.0337 - val_loss: 1.4595 - val_mean_absolute_error: 1.2004 - val_mean_squared_error: 1.4595\n",
      "Epoch 13/30\n",
      "Current Learning rate: 6.309573444801932e-05\n",
      "5000/5000 [==============================] - 64s 13ms/step - loss: 0.0293 - mean_absolute_error: 0.1329 - mean_squared_error: 0.0293 - val_loss: 0.0661 - val_mean_absolute_error: 0.2271 - val_mean_squared_error: 0.0661\n",
      "Epoch 14/30\n",
      "Current Learning rate: 5.011872336272723e-05\n",
      "5000/5000 [==============================] - 57s 11ms/step - loss: 0.0274 - mean_absolute_error: 0.1283 - mean_squared_error: 0.0274 - val_loss: 0.2166 - val_mean_absolute_error: 0.4471 - val_mean_squared_error: 0.2166\n",
      "Epoch 15/30\n",
      "Current Learning rate: 3.981071705534972e-05\n",
      "5000/5000 [==============================] - 89s 18ms/step - loss: 0.0262 - mean_absolute_error: 0.1251 - mean_squared_error: 0.0262 - val_loss: 0.6871 - val_mean_absolute_error: 0.8187 - val_mean_squared_error: 0.6871\n",
      "Epoch 16/30\n",
      "Current Learning rate: 3.1622776601683795e-05\n",
      "5000/5000 [==============================] - 52s 10ms/step - loss: 0.0240 - mean_absolute_error: 0.1193 - mean_squared_error: 0.0240 - val_loss: 0.9425 - val_mean_absolute_error: 0.9621 - val_mean_squared_error: 0.9425\n",
      "Epoch 17/30\n",
      "Current Learning rate: 2.51188643150958e-05\n",
      "5000/5000 [==============================] - 60s 12ms/step - loss: 0.0228 - mean_absolute_error: 0.1160 - mean_squared_error: 0.0228 - val_loss: 0.3524 - val_mean_absolute_error: 0.5801 - val_mean_squared_error: 0.3524\n",
      "Epoch 18/30\n",
      "Current Learning rate: 1.99526231496888e-05\n",
      "5000/5000 [==============================] - 52s 10ms/step - loss: 0.0217 - mean_absolute_error: 0.1134 - mean_squared_error: 0.0217 - val_loss: 0.1224 - val_mean_absolute_error: 0.3276 - val_mean_squared_error: 0.1224\n",
      "Epoch 19/30\n",
      "Current Learning rate: 1.5848931924611138e-05\n",
      "5000/5000 [==============================] - 58s 12ms/step - loss: 0.0211 - mean_absolute_error: 0.1113 - mean_squared_error: 0.0211 - val_loss: 0.9114 - val_mean_absolute_error: 0.9462 - val_mean_squared_error: 0.9114\n",
      "Epoch 20/30\n",
      "Current Learning rate: 1.2589254117941675e-05\n",
      "5000/5000 [==============================] - 53s 11ms/step - loss: 0.0211 - mean_absolute_error: 0.1109 - mean_squared_error: 0.0211 - val_loss: 1.1861 - val_mean_absolute_error: 1.0815 - val_mean_squared_error: 1.1861\n",
      "Epoch 21/30\n",
      "Current Learning rate: 1.0000000000000003e-05\n",
      "5000/5000 [==============================] - 59s 12ms/step - loss: 0.0207 - mean_absolute_error: 0.1103 - mean_squared_error: 0.0207 - val_loss: 0.2449 - val_mean_absolute_error: 0.4788 - val_mean_squared_error: 0.2449\n",
      "Epoch 22/30\n",
      "Current Learning rate: 7.943282347242817e-06\n",
      "5000/5000 [==============================] - 52s 10ms/step - loss: 0.0203 - mean_absolute_error: 0.1088 - mean_squared_error: 0.0203 - val_loss: 0.3226 - val_mean_absolute_error: 0.5538 - val_mean_squared_error: 0.3226\n",
      "Epoch 23/30\n",
      "Current Learning rate: 6.309573444801935e-06\n",
      "5000/5000 [==============================] - 57s 11ms/step - loss: 0.0197 - mean_absolute_error: 0.1070 - mean_squared_error: 0.0197 - val_loss: 0.7452 - val_mean_absolute_error: 0.8536 - val_mean_squared_error: 0.7452\n",
      "Epoch 24/30\n",
      "Current Learning rate: 5.011872336272722e-06\n",
      "5000/5000 [==============================] - 51s 10ms/step - loss: 0.0192 - mean_absolute_error: 0.1059 - mean_squared_error: 0.0192 - val_loss: 0.5703 - val_mean_absolute_error: 0.7442 - val_mean_squared_error: 0.5703\n",
      "Epoch 25/30\n",
      "Current Learning rate: 3.981071705534972e-06\n",
      "5000/5000 [==============================] - 59s 12ms/step - loss: 0.0191 - mean_absolute_error: 0.1054 - mean_squared_error: 0.0191 - val_loss: 0.2596 - val_mean_absolute_error: 0.4938 - val_mean_squared_error: 0.2596\n",
      "Epoch 26/30\n",
      "Current Learning rate: 3.1622776601683788e-06\n",
      "5000/5000 [==============================] - 50s 10ms/step - loss: 0.0190 - mean_absolute_error: 0.1053 - mean_squared_error: 0.0190 - val_loss: 0.4803 - val_mean_absolute_error: 0.6813 - val_mean_squared_error: 0.4803\n",
      "Epoch 27/30\n",
      "Current Learning rate: 2.5118864315095793e-06\n",
      "5000/5000 [==============================] - 58s 12ms/step - loss: 0.0187 - mean_absolute_error: 0.1040 - mean_squared_error: 0.0187 - val_loss: 0.4865 - val_mean_absolute_error: 0.6858 - val_mean_squared_error: 0.4865\n",
      "Epoch 28/30\n",
      "Current Learning rate: 1.9952623149688796e-06\n",
      "5000/5000 [==============================] - 48s 10ms/step - loss: 0.0186 - mean_absolute_error: 0.1035 - mean_squared_error: 0.0186 - val_loss: 0.4250 - val_mean_absolute_error: 0.6395 - val_mean_squared_error: 0.4250\n",
      "Epoch 29/30\n",
      "Current Learning rate: 1.5848931924611134e-06\n",
      "5000/5000 [==============================] - 55s 11ms/step - loss: 0.0186 - mean_absolute_error: 0.1034 - mean_squared_error: 0.0186 - val_loss: 0.5328 - val_mean_absolute_error: 0.7186 - val_mean_squared_error: 0.5328\n",
      "Epoch 30/30\n",
      "Current Learning rate: 1.2589254117941674e-06\n",
      "5000/5000 [==============================] - 52s 10ms/step - loss: 0.0183 - mean_absolute_error: 0.1031 - mean_squared_error: 0.0183 - val_loss: 0.5891 - val_mean_absolute_error: 0.7567 - val_mean_squared_error: 0.5891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2/3\n",
      "Epoch 1/30\n",
      "Current Learning rate: 0.001\n",
      "5000/5000 [==============================] - 63s 11ms/step - loss: 1652.4865 - mean_absolute_error: 5.7192 - mean_squared_error: 1652.4865 - val_loss: 6.6686 - val_mean_absolute_error: 2.4164 - val_mean_squared_error: 6.6686\n",
      "Epoch 2/30\n",
      "Current Learning rate: 0.0007943282347242815\n",
      "5000/5000 [==============================] - 48s 10ms/step - loss: 0.9772 - mean_absolute_error: 0.7739 - mean_squared_error: 0.9772 - val_loss: 40.7566 - val_mean_absolute_error: 6.3414 - val_mean_squared_error: 40.7566\n",
      "Epoch 3/30\n",
      "Current Learning rate: 0.0006309573444801933\n",
      "5000/5000 [==============================] - 58s 12ms/step - loss: 0.3821 - mean_absolute_error: 0.4734 - mean_squared_error: 0.3821 - val_loss: 80.8778 - val_mean_absolute_error: 8.9899 - val_mean_squared_error: 80.8778\n",
      "Epoch 4/30\n",
      "Current Learning rate: 0.0005011872336272722\n",
      "5000/5000 [==============================] - 47s 9ms/step - loss: 0.1727 - mean_absolute_error: 0.3174 - mean_squared_error: 0.1727 - val_loss: 7.3870 - val_mean_absolute_error: 2.7062 - val_mean_squared_error: 7.3870\n",
      "Epoch 5/30\n",
      "Current Learning rate: 0.00039810717055349724\n",
      "5000/5000 [==============================] - 54s 11ms/step - loss: 0.2618 - mean_absolute_error: 0.3221 - mean_squared_error: 0.2618 - val_loss: 22.2830 - val_mean_absolute_error: 4.7125 - val_mean_squared_error: 22.2830\n",
      "Epoch 6/30\n",
      "Current Learning rate: 0.00031622776601683794\n",
      "5000/5000 [==============================] - 46s 9ms/step - loss: 0.1196 - mean_absolute_error: 0.2552 - mean_squared_error: 0.1196 - val_loss: 6.1443 - val_mean_absolute_error: 2.4535 - val_mean_squared_error: 6.1443\n",
      "Epoch 7/30\n",
      "Current Learning rate: 0.000251188643150958\n",
      "5000/5000 [==============================] - 56s 11ms/step - loss: 0.0849 - mean_absolute_error: 0.2286 - mean_squared_error: 0.0849 - val_loss: 0.2139 - val_mean_absolute_error: 0.4274 - val_mean_squared_error: 0.2139\n",
      "Epoch 8/30\n",
      "Current Learning rate: 0.00019952623149688796\n",
      "5000/5000 [==============================] - 45s 9ms/step - loss: 0.0568 - mean_absolute_error: 0.1868 - mean_squared_error: 0.0568 - val_loss: 0.0753 - val_mean_absolute_error: 0.2308 - val_mean_squared_error: 0.0753\n",
      "Epoch 9/30\n",
      "Current Learning rate: 0.00015848931924611134\n",
      "5000/5000 [==============================] - 58s 12ms/step - loss: 0.0448 - mean_absolute_error: 0.1664 - mean_squared_error: 0.0448 - val_loss: 0.6469 - val_mean_absolute_error: 0.7897 - val_mean_squared_error: 0.6469\n",
      "Epoch 10/30\n",
      "Current Learning rate: 0.00012589254117941674\n",
      "5000/5000 [==============================] - 47s 9ms/step - loss: 0.0408 - mean_absolute_error: 0.1576 - mean_squared_error: 0.0408 - val_loss: 2.1263 - val_mean_absolute_error: 1.4482 - val_mean_squared_error: 2.1263\n",
      "Epoch 11/30\n",
      "Current Learning rate: 0.00010000000000000002\n",
      "3641/5000 [====================>.........] - ETA: 12s - loss: 0.0351 - mean_absolute_error: 0.1468 - mean_squared_error: 0.0351"
     ]
    }
   ],
   "source": [
    "num_runs = 3  # Number of times to run the model\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    model = create_model()\n",
    "    \n",
    "    model.compile(loss=loss_function[lf], optimizer = keras.optimizers.Adam(initial_lr), metrics=['mean_absolute_error','mean_squared_error'])\n",
    "    \n",
    "    MC_path = f\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Trainings/model_checkpoint_{step}_{run + 1}.h5\"\n",
    "    MC = ModelCheckpoint(\n",
    "    filepath=MC_path,  # Filepath to save the model weights\n",
    "    monitor='val_loss',  # Quantity to monitor (e.g., validation loss)\n",
    "    save_best_only=True,  # Save only the best model based on the monitored quantity\n",
    "    save_weights_only=True  # Save only the model weights, not the entire model\n",
    "    )\n",
    "\n",
    "    callbacks = [MC,LRS,CSV]\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(x=dataset_train, validation_data = dataset_val, steps_per_epoch=steps_per_epoch, epochs=num_epochs,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_runs = 3  # Number of times to run the model\n",
    "# partition = np.arange(0.1,0.9,0.1)\n",
    "\n",
    "# for step in partition:\n",
    "   \n",
    "#     eighty = round(step*file_num)\n",
    "#     twenty = file_num-eighty - num_test_files\n",
    "#     list_of_file_ids_train = np.arange(eighty, dtype=int)    \n",
    "#     list_of_file_ids_val = np.arange(eighty,eighty+twenty-num_test_files, dtype=int)\n",
    "    \n",
    "#     dataset_train = tf.data.Dataset.range(eighty).interleave(\n",
    "#         TrainDataset,\n",
    "#         cycle_length=2,\n",
    "#         num_parallel_calls=2,\n",
    "#         deterministic=True).repeat().prefetch(1)\n",
    "\n",
    "\n",
    "\n",
    "#     # Configuring training dataset\n",
    "#     dataset_val = tf.data.Dataset.range(twenty-num_test_files).interleave(\n",
    "#             ValDataset,\n",
    "#             cycle_length=2,\n",
    "#             num_parallel_calls=2,\n",
    "#             deterministic=True).prefetch(1)\n",
    "    \n",
    "#     print(f\"Step: {step}\")\n",
    "#     print(list_of_file_ids_train)\n",
    "#     print(list_of_file_ids_val) \n",
    "    \n",
    "#     for run in range(num_runs):\n",
    "#         print(f\"Run {run + 1}/{num_runs}\")\n",
    "\n",
    "\n",
    "#         model = create_model()\n",
    "\n",
    "#         model.compile(loss=loss_function[lf], optimizer = keras.optimizers.Adam(initial_lr), metrics=['mean_absolute_error','mean_squared_error'])\n",
    "\n",
    "#         MC_path = f\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Trainings/model_checkpoint_{step}_{run + 1}.h5\"\n",
    "#         MC = ModelCheckpoint(\n",
    "#         filepath=MC_path,  # Filepath to save the model weights\n",
    "#         monitor='val_loss',  # Quantity to monitor (e.g., validation loss)\n",
    "#         save_best_only=True,  # Save only the best model based on the monitored quantity\n",
    "#         save_weights_only=True  # Save only the model weights, not the entire model\n",
    "#         )\n",
    "\n",
    "#         callbacks = [MC,LRS,CSV]\n",
    "\n",
    "#         # Train the model\n",
    "#         history = model.fit(x=dataset_train, validation_data = dataset_val, steps_per_epoch=steps_per_epoch, epochs=num_epochs,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### All Analysis Tools:::\n",
    "\n",
    "# Loss/Val vs Epochs\n",
    "loss = history.history['loss']\n",
    "val = history.history['val_loss']\n",
    "\n",
    "x = np.arange(1,len(loss))\n",
    "\n",
    "# print(f'Training: {loss} \\n')\n",
    "# print(f'Validation: {val}')\n",
    "\n",
    "print(f'Train Final: {loss[-1]}')\n",
    "print(f'Val Final: {val[-1]}')\n",
    "\n",
    "# Full Training\n",
    "plt.plot(loss,label='Train')\n",
    "plt.title(f' Model Performance')\n",
    "plt.plot(val,label='Validation')\n",
    "plt.ylabel(f'{loss_function[lf]}')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Full Training\n",
    "plt.plot(loss,label='Train')\n",
    "plt.title(f' Model Performance')\n",
    "plt.plot(val,label='Validation')\n",
    "plt.ylabel(f'{loss_function[lf]}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Full training without first epoch\n",
    "plt.plot(x,loss[1:],label='Train')\n",
    "plt.plot(x,val[1:],label='Validation')\n",
    "plt.title(f' Model Performance')\n",
    "plt.ylabel(f'{loss_function[lf]}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Full training without first two epochs\n",
    "plt.plot(x[1:],loss[2:],label='Train')\n",
    "plt.plot(x[1:],val[2:],label='Validation')\n",
    "plt.title(f' Model Performance')\n",
    "plt.ylabel(f'{loss_function[lf]}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Full training without first three epochs\n",
    "plt.plot(x[2:],loss[3:],label='Train')\n",
    "plt.plot(x[2:],val[3:],label='Validation')\n",
    "plt.title(f' Model Performance')\n",
    "plt.ylabel(f'{loss_function[lf]}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bcd1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_path = f\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Trainings/MCPIN/model_checkpoint_1.h5\"\n",
    "model = create_model()\n",
    "model.load_weights(MC_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = []\n",
    "\n",
    "for i in list_of_file_ids_test:\n",
    "    # Load the data and perform necessary preprocessing steps\n",
    "    df = pd.read_hdf(file_list[i], key=None)\n",
    "    labels1 = df.iloc[:, 9].values\n",
    "    labels2 = df['ToF'].values\n",
    "    labels = labels1 + labels2\n",
    "    signals = df[df.columns[10:-2]].values\n",
    "    signals = Processing[process](signals)\n",
    "    signals = signals[:, :, np.newaxis]\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    predicted = model.predict(signals)\n",
    "    predicted = np.squeeze(predicted)\n",
    "    true = labels\n",
    "\n",
    "    # Calculate the test loss\n",
    "    test_loss = np.subtract(predicted, true)\n",
    "    avg_test_loss = np.mean(np.abs(test_loss))\n",
    "    avg_loss.append(avg_test_loss)\n",
    "\n",
    "# Calculate the mean and standard deviation of the average test loss\n",
    "mean_loss = np.mean(avg_loss)\n",
    "std_loss = np.std(avg_loss)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mean_loss}\")\n",
    "print(f\"Standard Deviation: {std_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd052f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = []\n",
    "\n",
    "for i in list_of_file_ids_test:\n",
    "    # Load the data and perform necessary preprocessing steps\n",
    "    df = pd.read_hdf(file_list[i], key=None)\n",
    "    labels1 = df.iloc[:, 9].values\n",
    "    labels2 = df['ToF'].values\n",
    "    labels = labels1 + labels2\n",
    "    signals = df[df.columns[10:-2]].values\n",
    "    signals = Processing[process](signals)\n",
    "    signals = signals[:, :, np.newaxis]\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    predicted = model.predict(signals)\n",
    "    predicted = np.squeeze(predicted)\n",
    "    true = labels\n",
    "\n",
    "    # Calculate the test loss\n",
    "    test_loss = np.subtract(predicted, true)\n",
    "\n",
    "    # Calculate mean squared error (MSE)\n",
    "    mse = np.mean(np.square(test_loss))\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    avg_loss.append(mse)\n",
    "\n",
    "# Calculate the mean and standard deviation of the average MSE\n",
    "mean_loss = np.mean(avg_loss)\n",
    "std_loss = np.std(avg_loss)\n",
    "\n",
    "print(f\"Mean Squared Error: {mean_loss}\")\n",
    "print(f\"Standard Deviation: {std_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccc3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([pd.read_hdf(file_list[i], key=None) for i in list_of_file_ids_test])\n",
    "unique_mass_test = df_test['A'].unique()\n",
    "\n",
    "result_dict = {}  # Dictionary to store unique_mass and corresponding avg_test_loss values\n",
    "\n",
    "# Test Set\n",
    "for mass_test in unique_mass_test:\n",
    "    print(f\"Unique Mass: {mass_test}\")\n",
    "    df_intermediate = df_test[df_test['A'] == mass_test]\n",
    "    unique_energy = df_intermediate['Z'].unique()\n",
    "\n",
    "    for energy in unique_energy:\n",
    "        print(f\"Unique Energy: {energy}\")\n",
    "        \n",
    "        df_combination = df_intermediate[df_intermediate['Z'] == energy]\n",
    "\n",
    "        labels1 = df_combination.iloc[:, 9].values\n",
    "        labels2 = df_combination['ToF'].values\n",
    "        labels = labels1 + labels2\n",
    "\n",
    "        signals = df_combination[df_combination.columns[10:-2]].values\n",
    "        signals = Processing[process](signals)\n",
    "        signals = signals[:, :, np.newaxis]\n",
    "\n",
    "        predicted = model.predict(signals)\n",
    "        predicted = np.squeeze(predicted)\n",
    "        true = labels\n",
    "\n",
    "        difference = predicted - true\n",
    "\n",
    "        # Plotting the distribution\n",
    "        plt.figure()\n",
    "        plt.hist(difference, bins=20)\n",
    "        plt.title(f\"A={mass_test}, Z={energy}, Count={len(df_combination)}\")\n",
    "        plt.xlabel(\"Difference (predicted - true)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "\n",
    "        # Calculate mean and standard deviation\n",
    "        mean = np.mean(difference)\n",
    "        std = np.std(difference)\n",
    "\n",
    "        # Create legend text with mean and standard deviation\n",
    "        legend_text = f\"Mean: {mean:.2f}\\nSTD: {std:.2f}\"\n",
    "\n",
    "        # Add legend with the mean and standard deviation\n",
    "        plt.legend([legend_text])\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        avg_test_loss = np.mean(np.abs(difference))\n",
    "        count = len(df_combination)\n",
    "\n",
    "        result_dict[(mass_test, energy)] = {\n",
    "            'avg_test_loss': avg_test_loss,\n",
    "            'count': count,\n",
    "            'mean_test_loss': mean,\n",
    "            'std_test_loss': std\n",
    "        }\n",
    "\n",
    "# Print the result dictionary\n",
    "for (mass, energy), result in result_dict.items():\n",
    "    avg_test_loss = result['avg_test_loss']\n",
    "    count = result['count']\n",
    "    mean_test_loss = result['mean_test_loss']\n",
    "    std_test_loss = result['std_test_loss']\n",
    "\n",
    "    print(f\"Unique Mass: {mass}, Unique Energy: {energy}\")\n",
    "    print(f\"Avg Test Loss: {avg_test_loss}\")\n",
    "    print(f\"Count: {count}\")\n",
    "    print(f\"Mean Test Loss: {mean_test_loss}\")\n",
    "    print(f\"Standard Deviation Test Loss: {std_test_loss}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e760a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data for scatter plots\n",
    "mass_list = []\n",
    "energy_list = []\n",
    "mean_loss_list = []\n",
    "std_loss_list = []\n",
    "\n",
    "for (mass, energy), result in result_dict.items():\n",
    "    mass_list.append(mass)\n",
    "    energy_list.append(energy)\n",
    "    mean_loss_list.append(result['mean_test_loss'])\n",
    "    std_loss_list.append(result['std_test_loss'])\n",
    "\n",
    "print(len(mass_list))\n",
    "print(len(energy_list))\n",
    "print(len(mean_loss_list))\n",
    "print(len(std_loss_list))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "# Extract the data for scatter plots\n",
    "mass_list = []\n",
    "energy_list = []\n",
    "mean_loss_list = []\n",
    "std_loss_list = []\n",
    "\n",
    "for (mass, energy), result in result_dict.items():\n",
    "    mass_list.append(mass)\n",
    "    energy_list.append(energy)\n",
    "    mean_loss_list.append(result['mean_test_loss'])\n",
    "    std_loss_list.append(result['std_test_loss'])\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Scatter plot for Mean Test Loss\n",
    "mean_loss_scatter = axs[0].scatter(mass_list, energy_list, c=mean_loss_list, cmap='coolwarm')\n",
    "axs[0].set_xlabel('Mass')\n",
    "axs[0].set_ylabel('Energy')\n",
    "axs[0].set_title('Scatter Plot: Mean Test Loss')\n",
    "mean_loss_cbar = plt.colorbar(mean_loss_scatter, ax=axs[0], label='Mean Test Loss')\n",
    "\n",
    "# Scatter plot for Standard Deviation\n",
    "std_loss_scatter = axs[1].scatter(mass_list, energy_list, c=std_loss_list, cmap='coolwarm')\n",
    "axs[1].set_xlabel('Mass')\n",
    "axs[1].set_ylabel('Energy')\n",
    "axs[1].set_title('Scatter Plot: Standard Deviation Test Loss')\n",
    "std_loss_cbar = plt.colorbar(std_loss_scatter, ax=axs[1], label='Standard Deviation Test Loss')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([pd.read_hdf(file_list[i], key=None) for i in list_of_file_ids_test])\n",
    "unique_mass_test = df_test['A'].unique()\n",
    "\n",
    "# Test Set\n",
    "for mass_test in unique_mass_test:\n",
    "    result_list = []  # List to store results as rows\n",
    "\n",
    "    df_intermediate = df_test[df_test['A'] == mass_test]\n",
    "    unique_energy = df_intermediate['Z'].unique()\n",
    "\n",
    "    for energy in unique_energy:\n",
    "        df_combination = df_intermediate[df_intermediate['Z'] == energy]\n",
    "\n",
    "        labels1 = df_combination.iloc[:, 9].values\n",
    "        labels2 = df_combination['ToF'].values\n",
    "        labels = labels1 + labels2\n",
    "\n",
    "        signals = df_combination[df_combination.columns[10:-2]].values\n",
    "        signals = Processing[process](signals)\n",
    "        signals = signals[:, :, np.newaxis]\n",
    "\n",
    "        predicted = model.predict(signals)\n",
    "        predicted = np.squeeze(predicted)\n",
    "        true = labels\n",
    "\n",
    "        difference = predicted - true\n",
    "        # Calculate mean and standard deviation\n",
    "        mean = np.mean(difference)\n",
    "        std = np.std(difference)\n",
    "\n",
    "        avg_test_loss = np.mean(np.abs(difference))\n",
    "        count = len(df_combination)\n",
    "\n",
    "        result_list.append([round(mass_test), round(energy), avg_test_loss, count, mean, std])\n",
    "\n",
    "    # Create the tabular representation for the current unique mass\n",
    "    headers = [\"Unique Mass\", \"Unique Energy\", \"Avg Test Loss\", \"Count\", \"Mean Test Loss\", \"Std Test Loss\"]\n",
    "    tabular_result = tabulate(result_list, headers=headers, floatfmt=\".3f\", tablefmt=\"github\")\n",
    "\n",
    "    # Print the tabular result for the current unique mass\n",
    "    print(f\"Tabular representation for Unique Mass: {mass_test}\")\n",
    "    print(tabular_result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef76580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group the data by unique mass and energy\n",
    "grouped_data = df_test.groupby(['A'])\n",
    "\n",
    "# Create an empty list to store the energies for each unique mass\n",
    "energies_per_mass = []\n",
    "\n",
    "# Iterate over each unique mass group\n",
    "for mass, group in grouped_data:\n",
    "    energies = group['Z'].unique()  # Get the unique energies for the current mass\n",
    "    energies_per_mass.append((mass, energies))  # Append the unique mass and energies tuple to the list\n",
    "\n",
    "# Create a list to store the average test losses for each unique mass\n",
    "avg_test_losses = []\n",
    "\n",
    "# Create an empty list to store the differences for each unique mass\n",
    "differences_per_mass = []\n",
    "\n",
    "# Iterate over the unique masses\n",
    "for i, (mass, energies) in enumerate(energies_per_mass, 1):\n",
    "    # Create a list to store the differences for the current mass\n",
    "    differences = []\n",
    "\n",
    "    # Iterate over the energies for the current mass\n",
    "    for energy in energies:\n",
    "        # Retrieve the data for the current mass and energy\n",
    "        data = df_test[(df_test['A'] == mass) & (df_test['Z'] == energy)]\n",
    "        \n",
    "        # Extract the features (signals) and labels (true values)\n",
    "        signals = data[data.columns[10:-2]].values\n",
    "        signals = Processing[process](signals)\n",
    "        signals = signals[:, :, np.newaxis]\n",
    "        labels1 = data.iloc[:, 9].values\n",
    "        labels2 = data['ToF'].values\n",
    "        labels = labels1 + labels2\n",
    "\n",
    "        # Predict the labels using the model\n",
    "        predicted = model.predict(signals)\n",
    "        predicted = np.squeeze(predicted)\n",
    "\n",
    "        # Calculate the difference between predicted and true labels\n",
    "        difference = predicted - labels\n",
    "        differences.append(difference)  # Store the difference for the current energy\n",
    "\n",
    "    differences_per_mass.append(differences)  # Store the differences for the current mass\n",
    "\n",
    "    # Calculate the average test loss for the current mass\n",
    "    avg_test_loss = np.mean(np.abs(np.concatenate(differences)))  # Concatenate the differences for all energies\n",
    "    avg_test_losses.append(avg_test_loss)  # Store the average test loss\n",
    "\n",
    "# Iterate over the unique masses and corresponding differences\n",
    "for i, (mass, differences) in enumerate(zip(energies_per_mass, differences_per_mass), 1):\n",
    "    energies = mass[1]  # Get the energies for the current mass\n",
    "\n",
    "    # Sort the energies in ascending order\n",
    "    sorted_energies = np.sort(energies)\n",
    "\n",
    "    # Get the indices to reorder the differences based on the sorted energies\n",
    "    indices = np.argsort(energies)\n",
    "    reordered_differences = [differences[index] for index in indices]\n",
    "\n",
    "    plt.figure(figsize=(150, 6))  # Adjust the figure size as needed\n",
    "    plt.subplot(1, len(energies_per_mass), i)  # Create a subplot for each unique mass\n",
    "    plt.boxplot(reordered_differences)\n",
    "    plt.title(f\"Unique Mass: {mass[0]}\")\n",
    "    plt.xlabel(\"Energy\")\n",
    "    plt.ylabel(\"Difference (Predicted - True)\")\n",
    "\n",
    "    # Set the x-axis tick labels to the sorted energies\n",
    "    plt.xticks(range(1, len(sorted_energies) + 1), sorted_energies)\n",
    "\n",
    "    plt.tight_layout()  # Adjust the spacing between subplots\n",
    "    plt.show()\n",
    "\n",
    "# Print the average test losses\n",
    "for i, (mass, _) in enumerate(energies_per_mass, 1):\n",
    "    print(f\"Unique Mass: {mass}, Average Test Loss: {avg_test_losses[i-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea241ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list to store the average test losses for each unique mass\n",
    "avg_test_losses = []\n",
    "\n",
    "# Create an empty list to store the differences for each unique mass\n",
    "differences_per_mass = []\n",
    "\n",
    "# Iterate over the unique masses\n",
    "for i, (mass, energies) in enumerate(energies_per_mass, 1):\n",
    "    # Create a list to store the differences for the current mass\n",
    "    differences = []\n",
    "\n",
    "    # Iterate over the energies for the current mass\n",
    "    for energy in energies:\n",
    "        # Retrieve the data for the current mass and energy\n",
    "        data = df_test[(df_test['A'] == mass) & (df_test['Z'] == energy)]\n",
    "        \n",
    "        # Extract the features (signals) and labels (true values)\n",
    "        signals = data[data.columns[10:-2]].values\n",
    "        signals = Processing[process](signals)\n",
    "        signals = signals[:, :, np.newaxis]\n",
    "        labels1 = data.iloc[:, 9].values\n",
    "        labels2 = data['ToF'].values\n",
    "        labels = labels1 + labels2\n",
    "\n",
    "        # Predict the labels using the model\n",
    "        predicted = model.predict(signals)\n",
    "        predicted = np.squeeze(predicted)\n",
    "\n",
    "        # Calculate the difference between predicted and true labels\n",
    "        difference = predicted - labels\n",
    "        differences.append(difference)  # Store the difference for the current energy\n",
    "\n",
    "    differences_per_mass.append(differences)  # Store the differences for the current mass\n",
    "\n",
    "    # Calculate the average test loss for the current mass\n",
    "    avg_test_loss = np.mean(np.abs(np.concatenate(differences)))  # Concatenate the differences for all energies\n",
    "    avg_test_losses.append(avg_test_loss)  # Store the average test loss\n",
    "\n",
    "# Iterate over the unique masses and corresponding differences\n",
    "for i, (mass, differences) in enumerate(zip(energies_per_mass, differences_per_mass), 1):\n",
    "    energies = mass[1]  # Get the energies for the current mass\n",
    "\n",
    "    # Sort the energies in ascending order\n",
    "    sorted_energies = np.sort(energies)\n",
    "\n",
    "    # Get the indices to reorder the differences based on the sorted energies\n",
    "    indices = np.argsort(energies)\n",
    "    reordered_differences = [differences[index] for index in indices]\n",
    "\n",
    "    plt.figure(figsize=(150, 6))  # Adjust the figure size as needed\n",
    "    plt.subplot(1, len(energies_per_mass), i)  # Create a subplot for each unique mass\n",
    "    plt.boxplot(reordered_differences)\n",
    "    plt.title(f\"Unique Mass: {mass[0]}\")\n",
    "    plt.xlabel(\"Energy\")\n",
    "    plt.ylabel(\"Difference (Predicted - True)\")\n",
    "\n",
    "    # Set the x-axis tick labels to the sorted energies\n",
    "    plt.xticks(range(1, len(sorted_energies) + 1), sorted_energies)\n",
    "\n",
    "    # Add a text box with counts for each energy\n",
    "    counts = [len(d) for d in reordered_differences]\n",
    "    text_box = \"\\n\".join([f\"E{e}: {count}\" for e, count in zip(sorted_energies, counts)])\n",
    "    plt.text(0.95, 0.95, text_box, transform=plt.gca().transAxes, va='top', ha='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()  # Adjust the spacing between subplots\n",
    "    plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bdf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([pd.read_hdf(file_list[i], key=None) for i in list_of_file_ids_test])\n",
    "unique_mass_test = df_test['A'].unique()\n",
    "\n",
    "result_list = []  # List to store results for all unique mass and energy combinations\n",
    "\n",
    "# Test Set\n",
    "for mass_test in unique_mass_test:\n",
    "    df_intermediate = df_test[df_test['A'] == mass_test]\n",
    "    unique_energy = df_intermediate['Z'].unique()\n",
    "\n",
    "    for energy in unique_energy:\n",
    "        df_combination = df_intermediate[df_intermediate['Z'] == energy]\n",
    "\n",
    "        labels1 = df_combination.iloc[:, 9].values\n",
    "        labels2 = df_combination['ToF'].values\n",
    "        labels = labels1 + labels2\n",
    "\n",
    "        signals = df_combination[df_combination.columns[10:-2]].values\n",
    "        signals = Processing[process](signals)\n",
    "        signals = signals[:, :, np.newaxis]\n",
    "\n",
    "        predicted = model.predict(signals)\n",
    "        predicted = np.squeeze(predicted)\n",
    "        true = labels\n",
    "\n",
    "        difference = predicted - true\n",
    "\n",
    "        # Calculate mean and standard deviation\n",
    "        mean = np.mean(difference)\n",
    "        std = np.std(difference)\n",
    "\n",
    "        avg_test_loss = np.mean(np.abs(difference))\n",
    "        count = len(df_combination)\n",
    "\n",
    "        result_list.append([mass_test, energy, avg_test_loss, count, mean, std])\n",
    "\n",
    "# Sort the results by avg_test_loss in ascending order\n",
    "result_list.sort(key=lambda x: x[2])\n",
    "\n",
    "# Print the 5 best cases\n",
    "best_cases = result_list[:5]\n",
    "\n",
    "for case in best_cases:\n",
    "    mass_test, energy, avg_test_loss, count, mean, std = case\n",
    "\n",
    "    # Plotting the distribution\n",
    "    df_combination = df_test[(df_test['A'] == mass_test) & (df_test['Z'] == energy)]\n",
    "    labels1 = df_combination.iloc[:, 9].values\n",
    "    labels2 = df_combination['ToF'].values\n",
    "    labels = labels1 + labels2\n",
    "\n",
    "    signals = df_combination[df_combination.columns[10:-2]].values\n",
    "    signals = Processing[process](signals)\n",
    "    signals = signals[:, :, np.newaxis]\n",
    "\n",
    "    predicted = model.predict(signals)\n",
    "    predicted = np.squeeze(predicted)\n",
    "    true = labels\n",
    "\n",
    "    difference = predicted - true\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(difference, bins=20)\n",
    "    ax.set_title(f\"Best Case - A={mass_test}, Z={energy}, Count={count}\")\n",
    "    ax.set_xlabel(\"Data\")\n",
    "    ax.set_ylabel(\"Difference (predicted - true)\")\n",
    "    ax.text(0.75, 0.9, f\"Mean: {mean:.2f}\\nSTD: {std:.2f}\", transform=ax.transAxes, bbox=dict(facecolor='white'))\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Unique Mass: {mass_test}, Unique Energy: {energy}\")\n",
    "    print(f\"Avg Test Loss: {avg_test_loss}\")\n",
    "    print(f\"Count: {count}\")\n",
    "    print(f\"Mean Test Loss: {mean}\")\n",
    "    print(f\"Standard Deviation Test Loss: {std}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([pd.read_hdf(file_list[i], key=None) for i in list_of_file_ids_test])\n",
    "unique_mass_test = df_test['A'].unique()\n",
    "\n",
    "result_list = []  # List to store results for all unique mass and energy combinations\n",
    "\n",
    "# Test Set\n",
    "for mass_test in unique_mass_test:\n",
    "    df_intermediate = df_test[df_test['A'] == mass_test]\n",
    "    unique_energy = df_intermediate['Z'].unique()\n",
    "\n",
    "    for energy in unique_energy:\n",
    "        df_combination = df_intermediate[df_intermediate['Z'] == energy]\n",
    "\n",
    "        labels1 = df_combination.iloc[:, 9].values\n",
    "        labels2 = df_combination['ToF'].values\n",
    "        labels = labels1 + labels2\n",
    "\n",
    "        signals = df_combination[df_combination.columns[10:-2]].values\n",
    "        signals = Processing[process](signals)\n",
    "        signals = signals[:, :, np.newaxis]\n",
    "\n",
    "        predicted = model.predict(signals)\n",
    "        predicted = np.squeeze(predicted)\n",
    "        true = labels\n",
    "\n",
    "        difference = predicted - true\n",
    "\n",
    "        # Calculate mean and standard deviation\n",
    "        mean = np.mean(difference)\n",
    "        std = np.std(difference)\n",
    "\n",
    "        avg_test_loss = np.mean(np.abs(difference))\n",
    "        count = len(df_combination)\n",
    "\n",
    "        result_list.append([mass_test, energy, avg_test_loss, count, mean, std])\n",
    "\n",
    "# Sort the results by avg_test_loss in ascending order\n",
    "result_list.sort(key=lambda x: x[2])\n",
    "\n",
    "# Print the 5 worst cases\n",
    "worst_cases = result_list[-5:]\n",
    "\n",
    "for case in worst_cases:\n",
    "    mass_test, energy, avg_test_loss, count, mean, std = case\n",
    "\n",
    "    # Plotting the distribution\n",
    "    df_combination = df_test[(df_test['A'] == mass_test) & (df_test['Z'] == energy)]\n",
    "    labels1 = df_combination.iloc[:, 9].values\n",
    "    labels2 = df_combination['ToF'].values\n",
    "    labels = labels1 + labels2\n",
    "\n",
    "    signals = df_combination[df_combination.columns[10:-2]].values\n",
    "    signals = Processing[process](signals)\n",
    "    signals = signals[:, :, np.newaxis]\n",
    "\n",
    "    predicted = model.predict(signals)\n",
    "    predicted = np.squeeze(predicted)\n",
    "    true = labels\n",
    "\n",
    "    difference = predicted - true\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(difference, bins=20)\n",
    "    ax.set_title(f\"Worst Case - A={mass_test}, Z={energy}, Count={count}\")\n",
    "    ax.set_xlabel(\"Data\")\n",
    "    ax.set_ylabel(\"Difference (predicted - true)\")\n",
    "    ax.text(0.75, 0.9, f\"Mean: {mean:.2f}\\nSTD: {std:.2f}\", transform=ax.transAxes, bbox=dict(facecolor='white'))\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Unique Mass: {mass_test}, Unique Energy: {energy}\")\n",
    "    print(f\"Avg Test Loss: {avg_test_loss}\")\n",
    "    print(f\"Count: {count}\")\n",
    "    print(f\"Mean Test Loss: {mean}\")\n",
    "    print(f\"Standard Deviation Test Loss: {std}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f309c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the test files into a single DataFrame\n",
    "df_test = pd.concat([pd.read_hdf(file_list[file_id], key=None) for file_id in list_of_file_ids_test])\n",
    "\n",
    "labels1 = df_test.iloc[:, 9].values\n",
    "labels2 = df_test['ToF'].values\n",
    "labels = labels1 + labels2\n",
    "\n",
    "signals = df_test[df_test.columns[10:-2]].values\n",
    "signals = Processing[process](signals)\n",
    "signals = signals[:, :, np.newaxis]\n",
    "\n",
    "# model.predict\n",
    "predicted = model.predict(signals)\n",
    "predicted = np.squeeze(predicted)  # getting rid of extra dimension\n",
    "true = labels\n",
    "\n",
    "difference = predicted - true\n",
    "# Your code to generate the histogram\n",
    "plt.hist(difference, bins=100)\n",
    "plt.title(\"Difference between Predicted and True Values\")\n",
    "plt.xlabel(\"Difference (Predicted - True) (ns)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean = np.mean(difference)\n",
    "std = np.std(difference)\n",
    "\n",
    "# Create legend text with mean and standard deviation\n",
    "legend_text = f\"Mean: {mean:.2f}\\nSTD: {std:.2f}\"\n",
    "\n",
    "# Add legend with the mean and standard deviation\n",
    "plt.legend([legend_text])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the test files into a single DataFrame\n",
    "df_test = pd.concat([pd.read_hdf(file_list[file_id], key=None) for file_id in list_of_file_ids_test])\n",
    "\n",
    "labels1 = df_test.iloc[:, 9].values\n",
    "labels2 = df_test['ToF'].values\n",
    "labels = labels1 + labels2\n",
    "\n",
    "signals = df_test[df_test.columns[10:-2]].values\n",
    "signals = Processing[process](signals)\n",
    "signals = signals[:, :, np.newaxis]\n",
    "\n",
    "# model.predict\n",
    "predicted = model.predict(signals)\n",
    "predicted = np.squeeze(predicted)  # getting rid of extra dimension\n",
    "true = labels\n",
    "\n",
    "difference = predicted - true\n",
    "# Plotting the histogram\n",
    "plt.hist(difference, bins=100)\n",
    "plt.title(\"Difference between Predicted and True Values\")\n",
    "plt.xlabel(\"Difference (Predicted - True) (ns)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.yscale('log')  # Set y-axis to logarithmic scale\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean = np.mean(difference)\n",
    "std = np.std(difference)\n",
    "\n",
    "# Create legend text with mean and standard deviation\n",
    "legend_text = f\"Mean: {mean:.2f}\\nSTD: {std:.2f}\"\n",
    "\n",
    "# Add legend with the mean and standard deviation\n",
    "plt.legend([legend_text])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
