{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb92245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this code I train my detector on both detectors and compare the results to\n",
    "training on a single detector and training on one and testing on the other.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import struct\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fef19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import struct\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2987c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "direction1 = pathlib.Path(\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Data/Processed/\")\n",
    "direction2 = pathlib.Path(\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Data/Processed2/\")\n",
    "\n",
    "file_list = list(direction1.iterdir())\n",
    "\n",
    "file_num = len(file_list)\n",
    "\n",
    "num_test_files = 2\n",
    "eighty = round(0.8*file_num)\n",
    "twenty = file_num-eighty - num_test_files\n",
    "\n",
    "list_of_file_ids_train = np.arange(eighty, dtype=int)\n",
    "\n",
    "list_of_file_ids_val = np.arange(eighty,eighty+twenty-num_test_files, dtype=int)\n",
    "\n",
    "list_of_file_ids_test =np.arange(file_num-num_test_files,file_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = pathlib.Path(\"C:/Users/steve/OneDrive/Bureaublad/Studies/Thesis/Data/Processed/\")\n",
    "\n",
    "file_list = list(direction.iterdir())\n",
    "\n",
    "file_num = len(file_list)\n",
    "\n",
    "num_test_files = 2\n",
    "eighty = round(0.8*file_num)\n",
    "twenty = file_num-eighty - num_test_files\n",
    "\n",
    "list_of_file_ids_train = np.arange(eighty, dtype=int)\n",
    "\n",
    "list_of_file_ids_val = np.arange(eighty,eighty+twenty-num_test_files, dtype=int)\n",
    "\n",
    "list_of_file_ids_test =np.arange(file_num-num_test_files,file_num)\n",
    "\n",
    "##### Functions to process the data\n",
    "\n",
    "        ### Unnormalization of each signal individually\n",
    "def Unnormalized(batch_signals):\n",
    "        \n",
    "        return batch_signals\n",
    "        \n",
    "        ### Normalization of each signal individually\n",
    "def Normalized(batch_signals):\n",
    "\n",
    "        for i in range(len(batch_signals)):\n",
    "            batch_signals[i] = batch_signals[i]/np.max(batch_signals[i])\n",
    "            \n",
    "        return batch_signals\n",
    "            \n",
    "        \n",
    "        ### Normalization of the entire value by one common denominator      \n",
    "def Denominator(batch_signals):  \n",
    "    \n",
    "        denominator = 3953.48\n",
    "        batch_signals = batch_signals/denominator\n",
    "        \n",
    "        return batch_signals\n",
    "\n",
    "\n",
    "##### Class\n",
    "\n",
    "class TrainDataset(tf.data.Dataset):\n",
    "\n",
    "    def _generator(file_id):  \n",
    "#         print(f'Using Train Class')\n",
    "        if(file_id == 0):\n",
    "#             print(\"reshuffling\")\n",
    "            np.random.shuffle(list_of_file_ids_train)             \n",
    "\n",
    "        i_file = list_of_file_ids_train[file_id]\n",
    "\n",
    "#         print(f'file_id: {file_id}, i_file: {i_file}')\n",
    "#         print()\n",
    "        signal_filename = direction/f'{i_file+1}.h5'\n",
    "\n",
    "        \n",
    "         # Load the labels and signals from the files\n",
    "        df = pd.read_hdf(signal_filename,key=None)    \n",
    "        \n",
    "        labels = df[variable]\n",
    "        signals = df[df.columns[9:]].values\n",
    "        \n",
    "        \n",
    "        # Determine how many batches can be made from this file\n",
    "        num_batches = len(signals) // batch_size\n",
    "\n",
    "        # Shuffle the signals within the file\n",
    "        signal_indices = np.arange(len(signals))\n",
    "        np.random.shuffle(signal_indices)        \n",
    "        \n",
    "        # Loop through each batch in the file\n",
    "        for batch_idx in range(num_batches):\n",
    "            # Get the signals and labels for this batch\n",
    "            batch_signal_indices = signal_indices[batch_idx*batch_size:(batch_idx+1)*batch_size]      \n",
    " \n",
    "            batch_signals = signals[batch_signal_indices]\n",
    "            \n",
    "            batch_signals = Processing[process](batch_signals)\n",
    "                \n",
    "            batch_signals = batch_signals[:,:,np.newaxis] # Can also be done with signals = signals[:,:,np.newaxis]\n",
    "            batch_labels = labels.iloc[batch_signal_indices]\n",
    "\n",
    "            # Yield the signals and labels as a tuple\n",
    "            yield batch_signals, batch_labels.values \n",
    "             \n",
    "    def __new__(cls, file_id):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=(tf.dtypes.float64, tf.dtypes.float64),\n",
    "            output_shapes=((batch_size, 2000,1), (batch_size, )),\n",
    "            args=(file_id,)\n",
    "        )\n",
    "    \n",
    "class ValDataset(tf.data.Dataset):\n",
    "\n",
    "    def _generator(file_id):  \n",
    "#         print(f'Using Val Class')\n",
    "        i_file = list_of_file_ids_val[file_id]\n",
    "    \n",
    "        signal_filename = direction/f'{i_file+1}.h5'\n",
    "\n",
    "         # Load the labels and signals from the files\n",
    "        df = pd.read_hdf(signal_filename,key=None)    \n",
    "        \n",
    "        labels = df[variable]\n",
    "        signals = df[df.columns[9:]].values\n",
    "        \n",
    "        \n",
    "        # Determine how many batches can be made from this file\n",
    "        num_batches = len(signals) // batch_size\n",
    "\n",
    "        # Shuffle the signals within the file\n",
    "        signal_indices = np.arange(len(signals))\n",
    "        np.random.shuffle(signal_indices)        \n",
    "        \n",
    "        # Loop through each batch in the file\n",
    "        for batch_idx in range(num_batches):\n",
    "            # Get the signals and labels for this batch\n",
    "            batch_signal_indices = signal_indices[batch_idx*batch_size:(batch_idx+1)*batch_size]      \n",
    " \n",
    "            batch_signals = signals[batch_signal_indices]\n",
    "            \n",
    "            batch_signals = Processing[process](batch_signals)\n",
    "                \n",
    "            batch_signals = batch_signals[:,:,np.newaxis] # Can also be done with signals = signals[:,:,np.newaxis]\n",
    "            batch_labels = labels.iloc[batch_signal_indices]\n",
    "\n",
    "            # Yield the signals and labels as a tuple\n",
    "            yield batch_signals, batch_labels.values \n",
    "             \n",
    "    def __new__(cls, file_id):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=(tf.dtypes.float64, tf.dtypes.float64),\n",
    "            output_shapes=((batch_size, 2000,1), (batch_size, )),\n",
    "            args=(file_id,)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
